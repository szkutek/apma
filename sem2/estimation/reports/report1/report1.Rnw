% TODO CHECK FOR MISTAKES

\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings
<<global_settings, echo=FALSE, warning=FALSE>>=
library(knitr)
# library(xtable) #package to create tables in LaTeX format
library(mvtnorm) # ex2
library(matlib) # ex2
library(plot3D)
# library(plotly)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Laboratory 1.}
\author{Agnieszka Szkutek, 208619 \\ Marta Frankowska, 208581}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}

We generate a vector of $Y\sim N(\mu=2, \sigma^2=4)$ with length $N = 1000$:
<<ex1.1, echo=TRUE, eval=FALSE>>=
Y <- rnorm(n = 1000, mean = 2, sd = 2)
# transform Y to Y1
Y1 <- 3 * (Y - 1) 
@
$Y_1$ has normal distribution, because it is a linear combination of $Y$. We can calculate analytical mean $\mu_1$ and variance $\sigma^2_1$:
\[ \mu_1 = E(Y_1) = E(3 (Y-1)) = 3 E(Y - 1) = 3 E(Y) - 3 = 3 \cdot 2 - 3 = 3\]
\[ \sigma^2_1 = Var(Y_1) = Var(3 (Y-1)) = 9 Var(Y) = 9 \cdot 4 = 36 \]

We can compare numerical and analytical results:
<<ex1.1mean, echo=TRUE, eval=TRUE>>=
Y <- rnorm(n = 1000, mean = 2, sd = 2)
Y1 <- 3 * (Y - 1) 
mean(Y1)
var(Y1)
@

Now we can plot the frequency historam of $Y_1$ and analytical normal density with $\mu=3$ and $\sigma^2=36$.
<<ex1.1hist, echo=FALSE, eval=TRUE>>=
Y <- rnorm(n = 1000, mean = 2, sd = 2)
Y1 <- 3 * (Y - 1) 
hist(Y1, freq = FALSE)
x <- seq(-20, 25, by = 0.01)
y <- dnorm(x, 3, 6)
lines(x, y, col = "red")
@

Next, we create variable $Y_2 = \left( \frac{Y-2}{2} \right)^2$.
$Y_2$ is a quadratic function of $Y$, so we know that distribution of $Y_2$ is not normal. What is more, 
\[\frac{Y-2}{2} \sim N(0,1).\] 
It means that $Y_2$, as a sum of squared standard normally distributed variables, has $\chi^2$ distribution with 1 degree of freedom.

<<ex1.2hist, echo=FALSE, eval=TRUE, fig.cap="Histogram and theoretical probability density function of $Y_2$" >>=
Y <- rnorm(n = 1000, mean = 2, sd = 2)
Y2 <- ((Y - 2) / 2) ^ 2
hist(Y2, freq = FALSE)
x <- seq(0, 120, by = 0.01)
y <- dchisq(x, 1)
lines(x, y, col = "red")
@

% EX 1.3

Next we will compute a sequence of means $m_n$ and a sequence of variances $\sigma_n^2$ for the variable~$Y$, where
\[ m_n = \frac{1}{n} \sum_{i=1}^{n} Y_i, \]
\[ \sigma_n = \frac{1}{n} \sum_{i=1}^{n} (Y_i - m_n)^2 \]
and plot the results.

<<ex1.2seq, echo=FALSE, eval=TRUE, fig.pos='H',fig.width=7, fig.height=4, fig.cap="Sequence of means and variances and their respective analytical values.">>=
nn = 1000
N <- seq(1, nn, by = 1)
Y <- rnorm(n = nn, mean = 2, sd = 2)
M <- cumsum(Y) / N
V <- cumsum((Y - M) ^ 2) / N

par(mfrow = c(1, 2))
plot(N, M)
lines(N, rep(2, length(N), type = "l", col = "red"))
plot(N, V)
lines(N, rep(4, length(N), type = "l", col = "red"))
@

The sequences $m_n$ and $\sigma_n^2$ converge to theoretical mean and variance, respectively. To examine the variability of the sequences we can calculate relative errors for both values.

\[ err_{m_n} = \left| \frac{m_n - \mu_n}{\mu_n}  \right| , \qquad
   err_{\sigma_n^2} = \left| \frac{\sigma_n^2 - \sigma_n}{\sigma_n}  \right| \]

<<ex1.2err, echo=FALSE, eval=TRUE, fig.pos='H',fig.width=7, fig.height=4, fig.cap="Relative errors for sequences of means and variances">>=
nn = 1000
N <- seq(1, nn, by = 1)
Y <- rnorm(n = nn, mean = 2, sd = 2)
M <- cumsum(Y) / N
V <- cumsum((Y - M) ^ 2) / N
errM  <- abs(M - 2) / 2
errV <- abs(V - 4) / 4
par(mfrow = c(1, 2))
plot(N, errM)
plot(N, errV)
@

For $N>200$ value of the error is less than $10\%$ of theoretical mean.
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2}

We simulate 10000 times and then plot 2-dimensional random variable $X\sim N(0,I_2)$.

<<ex2X, echo=FALSE, eval=TRUE, fig.cap="2-dimensional random variable $X$ of $N(0,I_2)$ distribution.">>=
n <- 10000
mu <- c(0, 0)
Sigma <- diag(2)
X <- rmvnorm(n = n, mean = mu, sigma =  Sigma)
plot(X)
@

We have to transform variable $X$ into variable $Y \sim N(\mu, \Sigma)$, where 
\[\mu = \left[0,\ 1\right] \quad \text{and }\quad 
  \Sigma = \left[
    \begin{matrix}  
      2   & 0.5 \\
      0.5 & 2
    \end{matrix} 
    \right]
\]
It means that we should find vector $a$ and matrix $A$, such that $Y=A X + a$. 
Expected value of $Y$ is equal to
\[E(Y) = E(AX+a) = A E(X)+a = a, \]
and variance
\[Var(Y) = Var(AX+a) = Var(AX) = A Var(X) A' = A I A' = A A' = \Sigma \]

We know that $\Sigma$ is a symmetric and positive definite matrix, so we can use Cholesky decomposition (chol() function in R) to obtain the value of $A$:
<<ex2.2chol, echo=TRUE, eval=TRUE>>=
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
chol(Sigma)
@

<<ex2.2Y, echo=FALSE, eval=FALSE, fig.cap="10000 realizations of the random variable $Y$">>=
n <- 10000
mu <- c(0, 0)
Sigma <- diag(2)
X <- rmvnorm(n = n, mean = mu, sigma =  Sigma)
mu = matrix(c(0, 1), 1, 2)
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
A <- chol(Sigma)
a <- mu
Y <- A %*% t(X) + t(apply(t(a), 1, rep, n))
plot(t(Y))
@


<<ex2checkDistrY2, echo=FALSE, eval=TRUE, fig.cap="Histograms and probability density functions of Y",fig.pos='H',fig.width=7, fig.height=4>>=
n <- 10000
mu <- c(0, 0)
Sigma <- diag(2)
X <- rmvnorm(n = n, mean = mu, sigma =  Sigma)
mu = matrix(c(0, 1), 1, 2)
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
A <- chol(Sigma)
a <- mu
Y <- A %*% t(X) + t(apply(t(a), 1, rep, n))

par(mfrow = c(1, 2))
p1.hist <- hist(Y[1,], freq = FALSE)
x <- seq(-5, 5, by = 0.01)
y <- dnorm(x, 0, sqrt(2))
p1.lines <- lines(x, y, col = "red")

p2.hist <- hist(Y[2,], freq = FALSE)
x <- seq(-5, 5, by = 0.01)
y <- dnorm(x, 1, sqrt(2))
p2.lines <- lines(x, y, col = "red")
@

Now we can plot the 3D histogram of the random variable $Y = \left[\begin{matrix} 1.414214 & 0.3535534 \\ 0.0 & 1.3693064 \end{matrix} \right]X+\mu$.
<<ex2.3hist3d, echo=FALSE, eval=TRUE, fig.cap="Random variable $Y$ and its 3D histogram with 30 bins and bars colored according to height.", fig.width=7>>=
n <- 10000
mu <- c(0, 0)
Sigma <- diag(2)
X <- rmvnorm(n = n, mean = mu, sigma =  Sigma)
mu = matrix(c(0, 1), 1, 2)
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
A <- chol(Sigma)
a <- mu
Y <- A %*% t(X) + t(apply(t(a), 1, rep, n))
Y1_c <- cut(Y[1, ], 5)
Y2_c <- cut(Y[2, ], 6)

par(mfrow = c(1, 2))
plot(t(Y))
hist3D(z = table(Y1_c, Y2_c), border = "black")
@

% exercise 2.4

We are going to transform the variable $Y$ into the variable $Z= (Y-\mu)' \Sigma^{-1} (Y-\mu)$, and $\Sigma$ is a non-singular matrix.
\[
Z = (Y-\mu)' \Sigma^{-1} (Y-\mu) = (Y-\mu)' \Sigma^{-0.5} \Sigma^{-0.5} (Y-\mu) = \left(\Sigma^{-0.5} (Y-\mu)\right)' \left(\Sigma^{-0.5} (Y-\mu)\right)
\]
Let's take $\Sigma^{-0.5} (Y-\mu) = B$. We know that $B \sim N(0,I)$, so $B'B \sim \chi^2(k)$, where $k=2$.


<<ex2.4histZ, echo=FALSE, eval=TRUE, fig.cap="Random variable $Z$ and its 3D histogram with 30 bins and bars colored according to height.", fig.width=7>>=
n <- 10000
mu <- c(0, 0)
Sigma <- diag(2)
X <- rmvnorm(n = n, mean = mu, sigma =  Sigma)
mu = matrix(c(0, 1), 1, 2)
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
A <- chol(Sigma)
a <- mu
Y <- A %*% t(X) + t(apply(t(a), 1, rep, n))
Sigma <- matrix(c(2, 0.5, 0.5, 2), 2, 2)
Z <- t(Y-t(apply(t(a), 1, rep, n))) %*% solve(Sigma) %*% (Y-t(apply(t(a), 1, rep, n)))
Z1_c <- cut(Z[, 1], 5)
Z2_c <- cut(Z[, 2], 6)

par(mfrow = c(1, 2))
plot(Z)
hist3D(z = table(Z1_c, Z2_c), border = "black")
@












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 3}


Let $\hat{\beta}$ be a sequence of estimators of a $(K\times 1)$ vector $\beta$, which is asymptotically normal with 
\[ \sqrt{N} (\hat{\beta} - \beta) \rightarrow^d N(0,\Sigma) .\]

% EX 3.1
\subsection{Part 1}

If $R \neq 0$ is an $(M\times K)$ matrix, the asymptotic distribution of $\sqrt{N} (R \hat{\beta} - R \beta) \rightarrow^? N(\mu, \sigma)$. We will compute the mean and the variance of $\sqrt{N} (R \hat{\beta} - R \beta)$.

\[ \mu = E\left(\sqrt{N} (R \hat{\beta} - R \beta) \right) = E\left(\sqrt{N} R( \hat{\beta} - \beta) \right) = R\ E\left(\sqrt{N} ( \hat{\beta} - \beta) \right) = 0 \]
and
\[ \sigma = Var\left(\sqrt{N} (R \hat{\beta} - R \beta)\right) = Var\left(\sqrt{N} R(\hat{\beta} - \beta)\right) 
  =\footnote{From task 2 point 2.}\ R\ Var\left(\sqrt{N} (\hat{\beta} -\beta)\right) R' = R \Sigma R',\]
so 
\[ \sqrt{N} (R \hat{\beta} - R \beta) \rightarrow^d N(0, R \Sigma R'),\ \text{for } R \neq 0. \]


% EX 3.2
\subsection{Part 2}

If $p \lim \hat{A} = A$ then the asymptotic distribution of $\sqrt{N}\hat{A}(\hat{\beta}-\beta) \rightarrow^? N(\mu, \sigma)$. 
We know that if $x_n \rightarrow^d x$ and $p \lim y_n = c$, $x_n y_n \rightarrow^d c x$, so

\[ \sqrt{N}\hat{A}(\hat{\beta}-\beta) \rightarrow^d \sqrt{N} A (\hat{\beta}-\beta) \rightarrow^d N(0, A\Sigma A').  \]


% EX 3.3
\subsection{Part 3}

We will prove that $N \left( \hat\beta-\beta \right)' \hat{\Sigma}^{-1} \left( \hat\beta-\beta \right) \rightarrow^d \chi^2 (K)$ if $\Sigma$ is a non-singular matrix and $p \lim \hat{\Sigma} = \Sigma$.

\begin{gather*}
               \sqrt{N} \left( \hat\beta-\beta \right)' \hat{\Sigma}^{-0.5} \hat{\Sigma}^{-0.5} \left( \hat\beta-\beta \right)\sqrt{N} \\
\downarrow^d \\
               \sqrt{N} \left( \hat\beta-\beta \right)' \Sigma^{-0.5} \Sigma^{-0.5} \left( \hat\beta-\beta \right)\sqrt{N} \\
=           \\
                \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right)' \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right).
\end{gather*}

Let 
\[C = \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right)' \right) \rightarrow\footnote{From task 3 part 1.} N(0, \underbrace{ \Sigma^{-0.5}\Sigma \Sigma^{-0.5} }_{I_K}) \]

We have $C' C$, where $ C \rightarrow^d N(0,\ I_K)$, so 
\[ C' C \rightarrow^d \chi^2 (K). \]


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
