\documentclass[12pt, a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 1}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}

\subsection{Part 1}
We generate a vector of $Y\sim N(\mu=2, \sigma^2=4)$ with length $N = 1000$ and transform it into variable $Y_1$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Y} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{)}
\hlcom{# transform Y to Y1}
\hlstd{Y1} \hlkwb{<-} \hlnum{3} \hlopt{*} \hlstd{(Y} \hlopt{-} \hlnum{1}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
$Y_1$ has normal distribution, because it is a linear combination of $Y$. We can calculate analytical mean $\mu_1$ and variance $\sigma^2_1$ as follows:
\[ \mu_1 = E(Y_1) = E(3 (Y-1)) = 3 E(Y - 1) = 3 E(Y) - 3 = 3 \cdot 2 - 3 = 3,\]
\[ \sigma^2_1 = Var(Y_1) = Var(3 (Y-1)) = 9 Var(Y) = 9 \cdot 4 = 36 \]
and compute them numerically:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Y} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlkwc{n} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{mean} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{)}
\hlstd{Y1} \hlkwb{<-} \hlnum{3} \hlopt{*} \hlstd{(Y} \hlopt{-} \hlnum{1}\hlstd{)}
\hlkwd{mean}\hlstd{(Y1)}
\end{alltt}
\begin{verbatim}
## [1] 3.290679
\end{verbatim}
\begin{alltt}
\hlkwd{var}\hlstd{(Y1)}
\end{alltt}
\begin{verbatim}
## [1] 36.98737
\end{verbatim}
\end{kframe}
\end{knitrout}
As expected, numerical and analytical results are quite similar.
Now we can plot the frequency histogram of $Y_1$ and analytical normal density with $\mu=3$ and $\sigma^2=36$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex1_1hist-1} 

}

\caption[Frequency historam of $Y_1$ and analytical density of $N(\mu=3,\ \sigma^2=36)$]{Frequency historam of $Y_1$ and analytical density of $N(\mu=3,\ \sigma^2=36)$.}\label{fig:ex1.1hist}
\end{figure}


\end{knitrout}


\subsection{Part 2}
Next, we create variable $Y_2 = \left( \frac{Y-2}{2} \right)^2$, which is a quadratic function of $Y$, so we know that distribution of $Y_2$ is not normal. What is more, 
\[\frac{Y-2}{2} \sim N(0,1).\] 
It means that $Y_2$, as a sum of squared standard normally distributed variables, has $\chi^2$ distribution with 1 degree of freedom.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex1_2hist-1} 

}

\caption[Histogram of $Y_2$ and theoretical probability density function of $\chi^2(1)$ distribution]{Histogram of $Y_2$ and theoretical probability density function of $\chi^2(1)$ distribution.}\label{fig:ex1.2hist}
\end{figure}


\end{knitrout}


\subsection{Part 3 and 4}
Next we will compute a sequence of means $m_n$ and a sequence of variances $\sigma_n^2$ for the variable~$Y$, where
\[ m_n = \frac{1}{n} \sum_{i=1}^{n} Y_i, \]
\[ v_n = \frac{1}{n} \sum_{i=1}^{n} (Y_i - m_n)^2 \]
and plot the results.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex1_2seq-1} 

}

\caption[Sequence of means and variances and their respective analytical values]{Sequence of means and variances and their respective analytical values.}\label{fig:ex1.2seq}
\end{figure}


\end{knitrout}

The sequences $m_n$ and $v_n^2$ converge to theoretical mean and variance, respectively. To examine the variability of the sequences we can calculate relative errors for both values.
\[ err_{m_n} = \left| \frac{m_n - \mu_n}{\mu_n}  \right| , \qquad
   err_{v_n} = \left| \frac{v_n - \sigma_n^2}{\sigma_n^2}  \right| \]

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex1_2err-1} 

}

\caption[Relative errors for sequences of means and variances]{Relative errors for sequences of means and variances.}\label{fig:ex1.2err}
\end{figure}


\end{knitrout}

We can observe that the values of both $err_{m_n}$ and $err_{\sigma^2}$ are bounded by 0.1.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 2}

\subsection{Part 1}
We simulate 10000 times and then plot 2-dimensional random variable $X\sim N(0,I_2)$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2X-1} 

}

\caption[2-dimensional random variable $X \sim N(0,I_2)$]{2-dimensional random variable $X \sim N(0,I_2)$.}\label{fig:ex2X}
\end{figure}


\end{knitrout}


\subsection{Part 2}
To transform variable $X$ into variable $Y \sim N(\mu, \Sigma)$, where 
\[\mu = \left[
    \begin{matrix}  
      0 \\
      1
    \end{matrix} 
    \right]
  \quad \text{and }\quad 
  \Sigma = \left[
    \begin{matrix}  
      2   & 0.5 \\
      0.5 & 2
    \end{matrix} 
    \right],
\]
we should find vector $a$ and matrix $A$, such that $Y=A X + a$. 
Expected value of $Y$ is equal to
\[E(Y) = E(AX+a) = A E(X)+a = a, \]
and variance
\[Var(Y) = Var(AX+a) = Var(AX) = A Var(X) A' = A I A' = A A' = \Sigma. \]
We know that $\Sigma$ is a symmetric and positive definite matrix, so we can use Cholesky decomposition (chol() function in R) to obtain the value of $A$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Sigma} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{0.5}\hlstd{,} \hlnum{2}\hlstd{),} \hlnum{2}\hlstd{,} \hlnum{2}\hlstd{)}
\hlkwd{chol}\hlstd{(Sigma)}
\end{alltt}
\begin{verbatim}
##          [,1]      [,2]
## [1,] 1.414214 0.3535534
## [2,] 0.000000 1.3693064
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2checkDistrY2-1} 

}

\caption[Histograms and probability density functions of Y]{Histograms and probability density functions of Y}\label{fig:ex2checkDistrY2}
\end{figure}


\end{knitrout}


\subsection{Part 3}
Now we can plot the 3D histogram of the random variable $Y = \left[\begin{matrix} 1.414214 & 0.3535534 \\ 0.0 & 1.3693064 \end{matrix} \right]X+\left[\begin{matrix}0\\1\end{matrix}\right]$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_3hist3d-1} 

}

\caption[Random variable $Y$ and its 3D histogram with 30 bins and bars colored according to height]{Random variable $Y$ and its 3D histogram with 30 bins and bars colored according to height.}\label{fig:ex2.3hist3d}
\end{figure}


\end{knitrout}


\subsection{Part 4}
We are going to transform the variable $Y$ into the variable $Z= (Y-\mu)' \Sigma^{-1} (Y-\mu)$, and $\Sigma$ is a non-singular matrix.
\[
Z = (Y-\mu)' \Sigma^{-1} (Y-\mu) = (Y-\mu)' \Sigma^{-0.5} \Sigma^{-0.5} (Y-\mu) = \left(\Sigma^{-0.5} (Y-\mu)\right)' \left(\Sigma^{-0.5} (Y-\mu)\right)
\]
Let's take $\Sigma^{-0.5} (Y-\mu) = B$. We know that $B \sim N(0,I)$, so $B'B \sim \chi^2(k)$, where $k=2$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_4histZ-1} 

}

\caption[Random variable $Z$ and its 3D histogram with 30 bins and bars colored according to height]{Random variable $Z$ and its 3D histogram with 30 bins and bars colored according to height.}\label{fig:ex2.4histZ}
\end{figure}


\end{knitrout}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 3}

Let $\hat{\beta}$ be a sequence of estimators of a $(K\times 1)$ vector $\beta$, which is asymptotically normal with 
\[ \sqrt{N} (\hat{\beta} - \beta) \rightarrow^d N(0,\Sigma) .\]



\subsection{Part 1}

If $R \neq 0$ is an $(M\times K)$ matrix, then $\sqrt{N} (R \hat{\beta} - R\beta) \rightarrow^d N(\mu, \sigma^2)$. We will compute the mean and the variance of $\sqrt{N} (R \hat{\beta} - R \beta)$:

\[ \mu = E\left(\sqrt{N} (R \hat{\beta} - R \beta) \right) = E\left(\sqrt{N} R( \hat{\beta} - \beta) \right) = R\ E\left(\sqrt{N} ( \hat{\beta} - \beta) \right) = 0 \]
and
\[ \sigma^2 = Var\left(\sqrt{N} (R \hat{\beta} - R \beta)\right) = Var\left(\sqrt{N} R(\hat{\beta} - \beta)\right) 
  =\footnote{From task 2 point 2.}\ R\ Var\left(\sqrt{N} (\hat{\beta} -\beta)\right) R' = R \Sigma R',\]
so 
\[ \sqrt{N} (R \hat{\beta} - R \beta) \rightarrow^d N(0, R \Sigma R'),\ \text{for } R \neq 0. \]



\subsection{Part 2}

If $p \lim \hat{A} = A$, then what does $\sqrt{N}\hat{A}(\hat{\beta}-\beta)$ converge to? 
We know that if $x_n \rightarrow^d x$ and $y_n \rightarrow^p c$, then $x_n y_n \rightarrow^d c x$, so
if 
\[ \hat{A} \rightarrow^p A \quad \text{and} \quad \sqrt{N}R(\hat{\beta} -\beta) \rightarrow^d N(0, R \Sigma R')\ \text{for } R \neq 0,\]
then
\[ \sqrt{N}\hat{A}(\hat{\beta}-\beta) \rightarrow^d N(0, A\Sigma A').  \]



\subsection{Part 3}

We will prove that $N \left( \hat\beta-\beta \right)' \hat{\Sigma}^{-1} \left( \hat\beta-\beta \right) \rightarrow^d \chi^2 (K)$ if $\Sigma$ is a non-singular matrix and $p \lim \hat{\Sigma} = \Sigma$.

\begin{gather*}
               N \left( \hat\beta-\beta \right)' \hat{\Sigma}^{-1} \left( \hat\beta-\beta \right) \\
=           \\               
               \sqrt{N} \left( \hat\beta-\beta \right)' \hat{\Sigma}^{-0.5} \hat{\Sigma}^{-0.5} \left( \hat\beta-\beta \right)\sqrt{N} \\
\downarrow^d \\
               \sqrt{N} \left( \hat\beta-\beta \right)' \Sigma^{-0.5} \Sigma^{-0.5} \left( \hat\beta-\beta \right)\sqrt{N} \\
=           \\
                \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right)' \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right).
\end{gather*}

From part 1 of task 3 we know that
\[\left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right) \rightarrow^d N(0, \underbrace{ \Sigma^{-0.5}\Sigma \Sigma^{-0.5} }_{I_K}). \]

Let $C = \left( \sqrt{N} \Sigma^{-0.5} \left( \hat\beta-\beta \right) \right)$.
We have $C' C$, where $ C \rightarrow^d N(0,\ I_K)$, so 
\[ C' C \rightarrow^d \chi^2 (K). \]


















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
