\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings
<<global_settings, echo=FALSE, warning=FALSE>>=
library(knitr)
library(xtable)
library(mvtnorm) 
library(matlib) 
library(plot3D)
library(readxl)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=7, fig.height=4)
@

\newtheorem{lemma}{Lemma}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 4}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 1}
We generate a time series of 20 observations according to the model $y_t = \alpha + \varepsilon_t$, where $\varepsilon_t \sim N(0,\sigma^2)$ and $iid$.

\subsection{Part 1}
The density function of a single observation is 
\[ f(y_t,\theta) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_t-\alpha)^2}{2\sigma^2}},  \]
the likelihood function is 
\begin{gather*} 
L(\theta; y_1,\dots,y_N) = f(y_1,\dots,y_N; \theta) = \prod_{i=1}^{N} f(y_i; \theta) = \\
= \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_1-\alpha)^2}{2\sigma^2}} \dots  \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_N-\alpha)^2}{2\sigma^2}}
=  \left(\frac{1}{\sqrt{2\pi\sigma^2}} \right)^N e^{-\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-\alpha)^2} ,
\end{gather*}
and the log-likelihood is as follows
\[ l(\theta; y_1,\dots,y_N) = \ln{L(\theta; y_1,\dots,y_N)} = -\frac{N}{2} \ln{2\pi} -\frac{N}{2} \ln{\sigma^2} -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-\alpha)^2.\]


\subsection{Part 2}
The contour plot of log-likelihood function is displayed below.
<<ex1.2, echo=FALSE, eval=TRUE>>=
alpha <- 0
sigma2 <- 4
Y <- rnorm(n = 20, mean = alpha, sd = sqrt(sigma2))

normal.loglik <- function(theta,y)
{
  log.f <- dnorm(y, mean = theta[1], sd = sqrt(theta[2]), log = TRUE)
  return(sum(log.f))
}

# plod 3D picture depend on alpha and sigma2
n.grid <- 200
alpha.grid <- seq(-1,1,length.out = n.grid)
sigma2.grid <- seq(2,6,length.out = n.grid)
alpha.sigma2.grid <- expand.grid(alpha.grid,sigma2.grid)

log.lik.values <- apply(alpha.sigma2.grid, MARGIN = 1, FUN = function(theta,y) normal.loglik(theta,y = Y), Y)
log.lik.value <- matrix(log.lik.values, nrow = n.grid, ncol = n.grid)

#contour plot
contour(alpha.grid, sigma2.grid, log.lik.value, nlevels = 50, xlab = "alpha", ylab = "sigma2")

alpha.est <- mean(Y)
sigma2.est <- sum((Y - mean(Y))^2)/length(Y)

points(alpha.est, sigma2.est, pch = 16, col = "red", cex = 3)
@


\subsection{Part 3}
The First Order Condition is as follows $\frac{\partial \ln L}{\partial \theta} $, where vector $\theta$ is equal to
$ \theta = 
  \begin{bmatrix}  
    \alpha\\
    \sigma^2
  \end{bmatrix} $.
% 
%
It gives the following set of equations
\begin{gather*}
\begin{cases}
  \frac{\partial \ln L}{\partial \alpha} = 0 \\
  \frac{\partial \ln L}{\partial \sigma^2} = 0 
  \end{cases}
\ \Rightarrow \ 
\begin{cases}
  \frac{1}{\sigma^2} \sum_{i=1}^N (y_i-\alpha) = 0 \\
  -\frac{N}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^N (y_i-\alpha)^2 = 0
\end{cases}
\ \Rightarrow \ 
\begin{cases}
  \frac{N}{N} \sum_{i=1}^N y_i - N \alpha = 0\\
  \frac{1}{\sigma^2} \sum_{i=1}^N (y_i-\alpha)^2 = N
\end{cases} \\
\ \Rightarrow \ 
\begin{cases}
  N (\bar{y}-\alpha) = 0\\
  \sigma^2 = \frac{1}{N} \sum_{i=1}^N (y_i-\alpha)^2
\end{cases}
\ \Rightarrow \ 
\begin{cases}
  \alpha = \bar{y} \\
  \sigma^2 = \frac{1}{N} \sum_{i=1}^N (y_i-\bar{y})^2
\end{cases}
\end{gather*}
%
So the ML estimators of the model parameters are
\[ \begin{cases}
  \hat{\alpha} = \bar{y}\\
  \hat{\sigma}^2 = \frac{1}{N} \sum_{i=1}^N (y_i-\bar{y})^2
\end{cases}
\]




\subsection{Part 4}
\subsubsection{Variance-covariance matrix}

The variance-covariance matrix of parameters $\hat{\alpha}$ and $\hat{\sigma}^{2}$:
\[
  \Sigma_{(\hat{\alpha},\hat{\sigma}^{2})}=
  \begin{bmatrix}  
    Var\hat{\alpha} & Cov(\hat{\alpha},\hat{\sigma}^{2})\\
    Cov(\hat{\alpha},\hat{\sigma}^{2}) & Var\hat{\sigma}^{2}
  \end{bmatrix},
\]
where $ Var\hat{\alpha}=Var\bar{y}=Var\left(\frac{1}{N}\sum^{N}_{i=1}y_{i}\right)=\frac{\sigma^{2}}{N} $.

We know that $\frac{N\hat{\sigma}^{2}}{\sigma^{2}}\sim\chi^{2}(N-1)$ and $Var[\chi^{2}(N-1)]=2(N-1)$ so
\[
Var\left(\frac{N\hat{\sigma}^{2}}{\sigma^{2}}\right)=\frac{N^2}{\sigma^{4}}Var(\hat{\sigma}^{2})=2(N-1)
\quad \Rightarrow \quad
Var(\hat{\sigma}^{2})=\frac{2(N-1)\sigma^{4}}{N^{2}}
\]
To calculate $Cov(\hat{\alpha},\hat{\sigma}^{2})$ we will use the following fact.
%
\begin{lemma}
Let $X_{1},...,X_{N}\sim N(\mu,\sigma^{2})$.
Then $\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_{i}$ and $\hat{\sigma}^{2}=\frac{1}{N}\sum_{i=1}^{N}(X_i-\bar{X})^{2}$ are independent. 
That means $Cov(\bar{X},\hat{\sigma}^{2})=0$.
\end{lemma}
%
Thus the variance-covariance matrix of parameters $\hat{\alpha}$ and $\hat{\sigma}^{2}$ is
\[
  \Sigma_{(\hat{\alpha},\hat{\sigma}^{2})}=
  \begin{bmatrix}  
    \sigma^{2} & 0\\
    0 & \frac{2(N-1)\sigma^{4}}{N^{4}}.
  \end{bmatrix}
\]


\subsubsection{The asymptotic distribution of ML estimator}

The asymptotic distribution of ML estimator is
\[\hat{\theta}\xrightarrow[]{d}N(\theta_0, I(\theta_0)^{-1}),\]
where $I(\theta_{0}) = -E_0(H(\theta_{0})) = -E_0\left(\frac{\partial^{2}\ln{L}}{\partial\theta_{0}\partial\theta_{0}'}\right)$.

We calculate partial derivatives:
\begin{align*}
\frac{\partial\ln{L}}{\partial\alpha}=\frac{1}{\sigma^2}N(\bar{y}-\alpha), & \quad \frac{\partial^{2}\ln{L}}{\partial\alpha^{2}}=-\frac{N}{\sigma^{2}} \\
\frac{\partial\ln{L}}{\partial\sigma^{2}}=-\frac{N}{2\sigma^{2}}+\frac{1}{2\sigma^{4}}\sum_{i=1}^{N}(y_{i}-\alpha)^{2}, & \quad
  \frac{\partial^{2}\ln{L}}{\partial(\sigma^{2})^{2}}=\frac{N}{2\sigma^{4}}-\frac{1}{(\sigma^{2})^{3}}\sum_{i=1}^{N}(y_{i}-\alpha)^2 \\
\frac{\partial^{2}\ln{L}}{\partial\alpha\partial\sigma^{2}} =\frac{\partial^{2}\ln{L}}{\partial\sigma^{2}\partial\alpha}=&-\frac{1}{\sigma^{4}}\sum_{i=1}^{N}(y_{i}-\alpha)
\end{align*}

Now we calculate expected value of the above derivatives:
\begin{align*}
E\frac{\partial^{2}\ln{L}}{\partial\alpha^{2}}=&-\frac{N}{\sigma^{2}} \\
%
E\frac{\partial^{2}\ln{L}}{\partial(\sigma^{2})^{2}}=&E\left(\frac{N}{2\sigma^{4}}\right)-E\left(\frac{1}{\sigma^{6}}\sum_{i=1}^{N}(y_{i}-\alpha)^{2}\right)=\frac{N}{2\sigma^{4}}-\frac{N}{\sigma^{4}}=-\frac{N}{2\sigma^{4}}\\
%
E\frac{\partial^{2}\ln{L}}{\partial\alpha\partial\sigma^{2}}=&-\frac{1}{\sigma^{4}}E\sum_{i=1}^{N}(y_{i}-\alpha)=-\frac{1}{\sigma^{4}}\sum_{i=1}^{N}E(y_{i}-\alpha)=0
\end{align*}
So 
\[
I^{-1}(\theta_{0})=
\begin{bmatrix}  
    \frac{N}{\sigma^{2}} & 0\\
    0 & \frac{N}{2\sigma^{4}}
  \end{bmatrix}^{-1}=
	\begin{bmatrix}  
    \frac{\sigma^{2}}{N} & 0\\
    0 & \frac{2\sigma^{4}}{N}
  \end{bmatrix}
\]
We see that asymptotic covariance of $\hat{\alpha}$ and $\hat{\sigma}^{2}$ is equal to 0.




\subsection{Part 5}

To calculate ML estimator of $1+\alpha+\alpha^{2}$ we use below property.
%
\begin{lemma}[Invariance property]
$\hat{g}(\theta)=g(\hat{\theta})$, where $\hat{\theta}$ is MLE of $\theta$ and $g$ is continous and continously differentaible funcion.
\end{lemma}
%
ML estimator of $\beta=1+\alpha+\alpha^{2}$ is 
\[\hat{\beta}=\hat{g}(\alpha)=g(\hat{\alpha})=1+\hat{\alpha}+\hat{\alpha}^{2}=1+\bar{y}+\bar{y}^{2}.\]
%
Now we calculate $Var\hat{\beta}$ using fact that $\hat{\alpha}\sim N(\alpha,\frac{\sigma^{2}}{N})$.
%
\begin{gather*}
Var(\hat{\beta})=Var(1+\hat{\alpha}+\hat{\alpha}^{2})=Var(\hat{\alpha}+\hat{\alpha}^{2})=E(\hat{\alpha}+\hat{\alpha}^{2})^{2}-(E(\hat{\alpha}+\hat{\alpha}^{2}))^{2}=\\
=E\hat{\alpha}^{2}+2E\hat{\alpha}^{3}+E\hat{\alpha}^{4}-(E\hat{\alpha})^{2}-2E\hat{\alpha}E\hat{\alpha}^{2}-(E\hat{\alpha}^{2})^{2}
\end{gather*}

Using formula for normal distribution moments we get:
\[ E\hat{\alpha}=\alpha ,\quad E\hat{\alpha}^{2}=\alpha^{2}+\frac{\sigma^{2}}{N} ,\quad E\hat{\alpha}^{3}=\alpha^{3}+3\alpha\frac{\sigma^{2}}{N} ,\quad\text{and}\quad
E\hat{\alpha}^{4}=\alpha^{4}+6\alpha^{2}\frac{\sigma^{2}}{N}+3\frac{\sigma^{4}}{N^{2}}
\]

\begin{gather*}
Var(\hat{\beta})=\alpha^{2}+\frac{\sigma^{2}}{N}+2\alpha^{3}+6\alpha\frac{\sigma^{2}}{N}+\alpha^{4}+6\alpha^{2}\frac{\sigma^{2}}{N}+3\frac{\sigma^{4}}{N^{2}}
-\alpha^{2}-2\alpha\left(\alpha^{2}-\frac{\sigma^{2}}{N}\right) -\left(\alpha^{2}+\frac{\sigma^{2}}{N}\right)^{2}= \\
= \frac{\sigma^{2}}{N}+4\alpha\frac{\sigma^{2}}{N}+4\alpha^{2}\frac{\sigma^{2}}{N}+2\frac{\sigma^4}{N^{2}}
\end{gather*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 2}
In this exercise we will be using data set from file \textit{datalab4-1.xlsx}. We will assume that $y$ has a mixed normal distribution,
\[ y_n \sim 
  \begin{cases}
    N(0,1), \quad\text{for } p \\
    N(\mu, \sigma^2), \quad\text{for } 1-p, \\
  \end{cases}
  \quad\text{depending on }
  \theta = \begin{bmatrix} \mu \\ \sigma^2 \\ p \end{bmatrix}.
\]

\subsection{Part 1}
The density function of $y$ is equal to
\[ f(y; \theta) = f(y; \mu, \sigma^2, p) = p\cdot  f(y; 0, 1) + (1-p) \cdot  f(y; \mu, \sigma^2), \]
where 
\[ f(y; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y-\mu)^2}{2\sigma^2}} \]
is the density function for normal distribution with parameters $\mu$ and $\sigma^2$.


The likelihood function of $y$ is obtained in the following way
\begin{gather*} 
  L = \prod_{i=1}^N f(y_i; \theta) = \prod_{i=1}^N  \left(  p\cdot  f(y_i; 0, 1) + (1-p) \cdot  f(y_i; \mu, \sigma^2) \right) = \\
  = \prod_{i=1}^N \left(  p\cdot  \frac{1}{\sqrt{2\pi}} e^{-\frac{y_i^2}{2}} + (1-p)\frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i-\mu)^2}{2\sigma^2}}  \right).
\end{gather*}


The log-likelihood function of $y$ is as follows


\subsection{Part 2}
<<ex2.2, echo=FALSE, eval=TRUE>>=
data1 <- read_excel('data_lab4-1.xlsx', col_names = FALSE)
y <- as.matrix(data1)

p <- 0.8
mi <- y[1]

normal.loglik <- function(sigma_2,y)
{
  log.f1 <- dnorm(y, mean = 0, sd = 1, log = TRUE)
  log.f2 <- dnorm(y, mean = mi, sd = sigma_2, log = TRUE)
  return(p*sum(log.f1) + (1-p)*sum(log.f2))
}

N = 2
sigma_2_vector <- 1:2*N
loglik <- 1:2*N
i <- 1

for (n in -N:N) {
  sigma_2_vector[i] <- 1/exp(2*n)
  loglik[i] <- normal.loglik(1/exp(2*n),y)
  i <- i + 1
}

plot(sigma_2_vector,loglik,col = 'black')
points(sigma_2_vector[which(loglik == max(loglik))],max(loglik),col = 'red',pch = 16)

@








\end{document}
