\documentclass[12pt, a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 2}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}


We have data file with 100 rows and 4 columns. We take the first column as a column vector $y$ and the remaining 3 columns as a matrix $X$, where $y$ depends on $X$.
We assume that the model for our data is as follows
\begin{equation} 
\label{eq1}
y = X \alpha + u. 
\end{equation}

We will use regression model function in R to compute the parameters and compare them with the results we obtain manually. 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{'data_lab_2.csv'}\hlstd{,} \hlkwc{sep} \hlstd{=} \hlstr{","}\hlstd{,} \hlkwc{dec} \hlstd{=} \hlstr{","}\hlstd{,} \hlkwc{header} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{attach}\hlstd{(data)}

\hlstd{N} \hlkwb{<-} \hlnum{100}
\hlstd{K} \hlkwb{<-} \hlnum{3}
\hlstd{X} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(data[,} \hlopt{-}\hlnum{1}\hlstd{])}
\hlstd{y} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(data[,} \hlnum{1}\hlstd{])}
\hlcom{# linear regression model using lm()}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(V1} \hlopt{~} \hlstd{.} \hlopt{-} \hlnum{1}\hlstd{, data)}

\hlkwd{summary}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = V1 ~ . - 1, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.68919 -0.47894  0.08483  0.47957  2.06775 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(>|t|)    
## V2  2.05860    0.06862   30.00   <2e-16 ***
## V3  1.07476    0.06720   15.99   <2e-16 ***
## V4  0.88974    0.07506   11.85   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.8956 on 97 degrees of freedom
## Multiple R-squared:  0.9951,	Adjusted R-squared:  0.9949 
## F-statistic:  6517 on 3 and 97 DF,  p-value: < 2.2e-16
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Part 1}
We want to express the loss function 
\[ L = \sum_{n=1}^{N} u_n^2 \]
as a function of $y$, $X$ and $\alpha$. From \eqref{eq1} we have $u=y-X\alpha$. Then

\begin{gather*} 
L = \sum_{n=1}^{N} u_n^2 = u' u = (y-X\alpha)' (y-X\alpha) = (y'-\alpha' X')(y-X\alpha) = \\
= y'y - y'X\alpha - \alpha'X'y + \alpha'X'X\alpha = y'y - 2y'X\alpha + \alpha'X'X\alpha.
\end{gather*}


\subsection{Part 2}
Next we will use the following equalities
\[ \frac{\partial A\beta}{\partial \beta'} = A \qquad \text{and} \qquad  \frac{\partial \beta' A\beta}{\partial \beta'} = \beta'(A+A')\]
to calculate the first derivative of $L$ with respect to $\alpha$.

\[ \frac{\partial L(\alpha)}{\partial \alpha'} = 0 - 2y'X + \alpha'(X'X + X'X) = - 2y'X +2 \alpha'X'X. \]


\subsection{Part 3} \label{referencja}
Now, to minimize the $L$ function, we will solve the first order condition equation $\frac{\partial L(\alpha)}{\partial \alpha'} = 0 $.

\begin{align*}
\frac{\partial L(\alpha)}{\partial \alpha'} & = 0 \\
-2 y'X + 2 \alpha'X'X  & = 0 \\
\alpha'X'X & = y'X \ \ /\cdot (X'X)^{-1} \\
\alpha' & = y'X (X'X)^{-1} \\
\alpha & = (X'X)^{-1} X'y
\end{align*}
So $\hat{\alpha}  = (X'X)^{-1} X'y$ is the LS estimator of the model parameter.

The first vector is the theoretical estimator of $\alpha$ and the second is the estimator obtained with R's linear regression model:
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Sun Nov 12 17:51:33 2017
\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \hline
 &   & theoretical & using lm() \\ 
  \hline
1 & alpha\_1 & 2.05859906656869 & 2.05859906656869 \\ 
  2 & alpha\_2 & 1.07475575539487 & 1.07475575539487 \\ 
  3 & alpha\_3 & 0.889740635967315 & 0.889740635967315 \\ 
   \hline
\end{tabular}
\caption{Estimator of alpha} 
\label{tab:alpha.est}
\end{table}




\subsection{Part 4}
We are using unbiased estimator for the variance of residuals
\[ \hat{\sigma}^2 = \frac{u'u}{N-K},\]
where in our case $N=100$ and $K=3$. 
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Sun Nov 12 17:51:33 2017
\begin{table}[H]
\centering
\begin{tabular}{rrr}
  \hline
 & theoretical & using lm() \\ 
  \hline
1 & 0.80202 & 0.80210 \\ 
   \hline
\end{tabular}
\caption{Variance of residuals} 
\label{tab:table1}
\end{table}

The first row is the theoretical estimator of $\sigma$ and the second is the squared residual standard error obtained with R's linear regression model.



\subsection{Part 5}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/ex1_resid_white_noise-1} 

}



\end{knitrout}

We assume that the residuals are uncorrelated and homoscedastic. The variance-covariance matrix of LS estimator is
\[ \hat{\Sigma}_{\hat{\alpha}} = \hat{\sigma}^2 (X'X)^{-1}, \]
where $\hat{\sigma}^2$ is the variance of the residuals.

We can calculate the variance-covariance matrix using R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##              V2           V3           V4
## V2  0.004708172 -0.002019469 -0.002544603
## V3 -0.002019469  0.004516371 -0.002430459
## V4 -0.002544603 -0.002430459  0.005633808
##              V2           V3           V4
## V2  0.004708172 -0.002019469 -0.002544603
## V3 -0.002019469  0.004516371 -0.002430459
## V4 -0.002544603 -0.002430459  0.005633808
\end{verbatim}
\end{kframe}
\end{knitrout}
where the first result is the theoretical estimator of $\Sigma$ and the second is the one obtained with R's linear regression model.

Variance-covariance matrix for $\sqrt{N}\hat{\alpha}$ is equal to:
\[ \hat{\Sigma}_{\sqrt{N}\hat{\alpha}} = N\hat{\sigma}^2 (X'X)^{-1}\]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##            V2         V3         V4
## V2  0.4708172 -0.2019469 -0.2544603
## V3 -0.2019469  0.4516371 -0.2430459
## V4 -0.2544603 -0.2430459  0.5633808
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Part 6}
The $t$-statistic tests the hypothesis $H_0:\ \alpha_i = 0$, $H_1:\ \alpha_i \neq 0$. The $t$-ratio is the ratio of the sample regression coefficient to its standard error. So
\[ t_{\hat{\alpha_i}} = \frac{\hat{\alpha_i}}{\sqrt{Var\hat{\alpha_i}}}, \]
where $t_{\hat{\alpha_i}} \sim t(N-K) = t(100-3) = t(97)$.

% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Sun Nov 12 17:51:34 2017
\begin{table}[H]
\centering
\begin{tabular}{rlll}
  \hline
 &   & theoretical & using lm() \\ 
  \hline
1 & V1 & 30.0016825108455 & 30.0016825108456 \\ 
  2 & V2 & 15.9924488168616 & 15.9924488168616 \\ 
  3 & V3 & 11.8539317172235 & 11.8539317172235 \\ 
   \hline
\end{tabular}
\caption{t-ratios of the parameters} 
\label{tab:table2}
\end{table}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 2}

In this exercise we assume that $\alpha_1 + \alpha_2 + \alpha_3 = 0$ and $\alpha_2 - \alpha_3 = 0$. 

\subsection{Part 1}
We know that the restriction matrix $R$ satisfies equation $R \alpha = r$. In this case 
\[ 
  R \cdot
  \begin{bmatrix}  
      \alpha_1 \\ \alpha_2 \\ \alpha_3
  \end{bmatrix} 
  =
  \begin{bmatrix}  
    0 \\ 0
  \end{bmatrix} 
  \quad \Rightarrow \quad
  R = 
  \begin{bmatrix}  
    1 & 1 & 1 \\
    0 & 1 & -1
  \end{bmatrix}
\]
Rank of the restriction matrix is $rank(R) = 2$, because there are two linearly independent vectors in matrix $R$, $[1,1,1]$ and $[0,1,-1]$.


\subsection{Part 2}
To express vector $\alpha$ and the loss function $L$ as functions of $\alpha_3$, we will first express $\alpha_1$ and $\alpha_2$ as functions of $\alpha_3$.

\[ 
\begin{cases}
  \alpha_1 + \alpha_2 + \alpha_3 = 0 \\
  \alpha_2 - \alpha_3 = 0
\end{cases}
\quad \Leftrightarrow \quad
\begin{cases}
  \alpha_1 = - \alpha_2 - \alpha_3 \\
  \alpha_2 = \alpha_3
\end{cases}
\quad \Leftrightarrow \quad
\begin{cases}
  \alpha_1 = - 2 \alpha_3 \\
  \alpha_2 = \alpha_3
\end{cases}
\]

So 
\[
  \alpha = 
  \begin{bmatrix}  
      \alpha_1 \\ \alpha_2 \\ \alpha_3
  \end{bmatrix} 
  =
  \begin{bmatrix}  
      -2 \alpha_3 \\ \alpha_3 \\ \alpha_3
  \end{bmatrix} 
  =
  \alpha_3
  \begin{bmatrix}  
      -2 \\ 1 \\ 1
  \end{bmatrix} 
\]

and

\begin{gather*}
L(\alpha_3) = y'y - 2y'X\alpha + \alpha'X'X\alpha = 
  y'y - 2y'X\alpha_3 \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + \alpha_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \alpha_3 \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  = \\
  y'y - 2\alpha_3 y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + \alpha_3^2 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix}
\end{gather*}



\subsection{Part 3}
Estimator of $\alpha_3$ is equal to 
\[ \hat{\alpha}_3 = \text{arg}\min_{\alpha_3\ \ } L(\alpha_3) \]

Again, we will use the formula $\frac{\partial A\beta}{\partial \beta'} = A$ to calculate 
\[ \frac{\partial L(\alpha_3)}{\partial \alpha_3} = 
  0 - 2 y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + 2 \alpha_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
\]
From F.O.C and the above formula we obtain 
\begin{align*}
  \hat{\alpha}_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  & = y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \\
  \hat{\alpha}_3 & =
    y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \left(\begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \right)^{-1}\\
\end{align*}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 3}



Next, we consider the following model
\[y_{n} = \alpha_{1}X_{1n}+\alpha_{2}X_{2n}+\varepsilon_{n}\]
with $\alpha_{1} = \alpha_{2} = 1$ and $\varepsilon_{n}\sim N(0,1)$.



\subsection{Part 1}
We will generate samples from above model in two cases:
\begin{enumerate}
\item[a)] $X_{1}$ and $X_{2}$ are independent and $X \sim N(0,I_{2})$,
\item[b)] $X_{1}$ and $X_{2}$ are dependent and $X \sim N(0,\Sigma)$.
\end{enumerate}


{\centering \includegraphics[width=\maxwidth]{figure/ex3_1-1} 

}






\subsection{Part 2 and 3}
We have the following models: 
\[ y_{n} = \alpha_{1}X_{1n}+\alpha_{2}X_{2n}+\varepsilon_{n} \]
\[ y_{n} = \beta X_{1n}+u_{n} \]
We will use the formula from subsection \ref{referencja} to obtain $\alpha_{1}$ and $\beta$ in those models. 
Below we compare the results for different sample sizes.
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Sun Nov 12 17:51:36 2017
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & N & theoretical\_alpha\_1 & alpha\_1\_est & beta\_est \\ 
  \hline
1 & 10.00000 & 1.00000 & 1.01685 & 1.00558 \\ 
  2 & 100.00000 & 1.00000 & 0.99905 & 1.00060 \\ 
  3 & 1000.00000 & 1.00000 & 1.00147 & 1.00059 \\ 
   \hline
\end{tabular}
\caption{Estimator of alpha1 and beta for model a} 
\end{table}
% latex table generated in R 3.2.3 by xtable 1.8-2 package
% Sun Nov 12 17:51:36 2017
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & N & theoretical\_alpha\_1 & alpha\_1\_est & beta\_est \\ 
  \hline
1 & 10.00000 & 1.00000 & 1.00154 & 1.25233 \\ 
  2 & 100.00000 & 1.00000 & 0.99924 & 1.25446 \\ 
  3 & 1000.00000 & 1.00000 & 1.00023 & 1.25047 \\ 
   \hline
\end{tabular}
\caption{Estimator of alpha1 and beta for model a} 
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
