\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings
<<global_settings, echo=FALSE, warning=FALSE>>=
library(knitr)
library(xtable)
library(mvtnorm) 
library(matlib) 
library(plot3D)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 2}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}


We have data file with 100 rows and 4 columns. We take the first column as a column vector $y$ and the remaining 3 columns as a matrix $X$, where $y$ depends on $X$.
We assume that the model for our data is as follows
\begin{equation} 
\label{eq1}
y = X \alpha + u. 
\end{equation}

We will use regression model function in R to compute the parameters and compare them with the results we obtain manually. 
<<ex1summary, echo=TRUE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
attach(data)

N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
# linear regression model using lm()
model <- lm(V1 ~ . - 1, data)

summary(model)
@

\subsection{Part 1}
We want to express the loss function 
\[ L = \sum_{n=1}^{N} u_n^2 \]
as a function of $y$, $X$ and $\alpha$. From \eqref{eq1} we have $u=y-X\alpha$. Then

\begin{gather*} 
L = \sum_{n=1}^{N} u_n^2 = u' u = (y-X\alpha)' (y-X\alpha) = (y'-\alpha' X')(y-X\alpha) = \\
= y'y - y'X\alpha - \alpha'X'y + \alpha'X'X\alpha = y'y - 2y'X\alpha + \alpha'X'X\alpha.
\end{gather*}


\subsection{Part 2}
Next we will use the following equalities
\[ \frac{\partial A\beta}{\partial \beta'} = A \qquad \text{and} \qquad  \frac{\partial \beta' A\beta}{\partial \beta'} = \beta'(A+A')\]
to calculate the first derivative of $L$ with respect to $\alpha$.

\[ \frac{\partial L(\alpha)}{\partial \alpha'} = 0 - 2y'X + \alpha'(X'X + X'X) = - 2y'X +2 \alpha'X'X. \]


\subsection{Part 3} \label{referencja}
Now, to minimize the $L$ function, we will solve the first order condition equation $\frac{\partial L(\alpha)}{\partial \alpha'} = 0 $.

\begin{align*}
\frac{\partial L(\alpha)}{\partial \alpha'} & = 0 \\
-2 y'X + 2 \alpha'X'X  & = 0 \\
\alpha'X'X & = y'X \ \ /\cdot (X'X)^{-1} \\
\alpha' & = y'X (X'X)^{-1} \\
\alpha & = (X'X)^{-1} X'y
\end{align*}
So $\hat{\alpha}  = (X'X)^{-1} X'y$ is the LS estimator of the model parameter.

The first vector is the theoretical estimator of $\alpha$ and the second is the estimator obtained with R's linear regression model:
<<ex1.alpha.est, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

# theoretical result:
alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y
# r model
model <- lm(V1 ~ . - 1, data)
alpha.model.est <- summary(model)$coefficients[, 1]
data.names <- c("alpha_1","alpha_2","alpha_3")
mat1 <- matrix(c(data.names, alpha.est, alpha.model.est), nrow=length(alpha.est))
tab1 <- xtable(mat1, digits = 5, row.names = FALSE, 
               caption = "Estimator of alpha", label = "tab:alpha.est")
names(tab1) <- c(" ", "theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@



\subsection{Part 4}
We are using unbiased estimator for the variance of residuals
\[ \hat{\sigma}^2 = \frac{u'u}{N-K},\]
where in our case $N=100$ and $K=3$. 
<<ex1.resid.var, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals
# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)
# summary(model) # Residual standard error squared

tab1 <- xtable(matrix(c(sigma2.est.unbiased, 0.8956 ^ 2), 1, 2), 
               digits = 5, row.names = FALSE, 
               caption = "Variance of residuals", label = "tab:table1")
names(tab1) <- c("theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@
The first row is the theoretical estimator of $\sigma$ and the second is the squared residual standard error obtained with R's linear regression model.



\subsection{Part 5}

<<ex1.resid.white.noise, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

plot(res)
@

We assume that the residuals are uncorrelated and homoscedastic. The variance-covariance matrix of LS estimator is
\[ \hat{\Sigma}_{\hat{\alpha}} = \hat{\sigma}^2 (X'X)^{-1}, \]
where $\hat{\sigma}^2$ is the variance of the residuals.

We can calculate the variance-covariance matrix using R:
<<ex1.resid.matrix, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y

model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)

# VARIANCE-COVARIANCE MATRIX OF LS ESTIMATOR
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators
cov.alpha.est
vcov(model)
@
where the first result is the theoretical estimator of $\Sigma$ and the second is the one obtained with R's linear regression model.

Variance-covariance matrix for $\sqrt{N}\hat{\alpha}$ is equal to:
\[ \hat{\Sigma}_{\sqrt{N}\hat{\alpha}} = N\hat{\sigma}^2 (X'X)^{-1}\]
<<ex1.resid.matrix.N, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y

model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)

# VARIANCE-COVARIANCE MATRIX OF LS ESTIMATOR
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators
cov.alpha.est.N <- N * cov.alpha.est
cov.alpha.est.N
@



\subsection{Part 6}
The $t$-statistic tests the hypothesis $H_0:\ \alpha_i = 0$, $H_1:\ \alpha_i \neq 0$. The $t$-ratio is the ratio of the sample regression coefficient to its standard error. So
\[ t_{\hat{\alpha_i}} = \frac{\hat{\alpha_i}}{\sqrt{Var\hat{\alpha_i}}}, \]
where $t_{\hat{\alpha_i}} \sim t(N-K) = t(100-3) = t(97)$.

<<ex1.t.ratios, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals
sigma2.est.unbiased <- t(res) %*% res / (N - K) # variance of residuals
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators

se.alpha.est <- sqrt(diag(cov.alpha.est))
t.ratio.theoretical <- alpha.est / se.alpha.est
t.ratio.empirical <- summary(model)$coefficients[, 3]


mat <- matrix(c("V1", "V2", "V3", t.ratio.theoretical, t.ratio.empirical), nrow=length(t.ratio.theoretical)) 
tab1 <- xtable(mat, digits = 3, row.names = FALSE,
               caption = "t-ratios of the parameters", label = "tab:table2")
names(tab1) <- c(" ", "theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 2}

In this exercise we assume that $\alpha_1 + \alpha_2 + \alpha_3 = 0$ and $\alpha_2 - \alpha_3 = 0$. 

\subsection{Part 1}
We know that the restriction matrix $R$ satisfies equation $R \alpha = r$. In this case 
\[ 
  R \cdot
  \begin{bmatrix}  
      \alpha_1 \\ \alpha_2 \\ \alpha_3
  \end{bmatrix} 
  =
  \begin{bmatrix}  
    0 \\ 0
  \end{bmatrix} 
  \quad \Rightarrow \quad
  R = 
  \begin{bmatrix}  
    1 & 1 & 1 \\
    0 & 1 & -1
  \end{bmatrix}
\]
Rank of the restriction matrix is $rank(R) = 2$, because there are two linearly independent vectors in matrix $R$, $[1,1,1]$ and $[0,1,-1]$.


\subsection{Part 2}
To express vector $\alpha$ and the loss function $L$ as functions of $\alpha_3$, we will first express $\alpha_1$ and $\alpha_2$ as functions of $\alpha_3$.

\[ 
\begin{cases}
  \alpha_1 + \alpha_2 + \alpha_3 = 0 \\
  \alpha_2 - \alpha_3 = 0
\end{cases}
\quad \Leftrightarrow \quad
\begin{cases}
  \alpha_1 = - \alpha_2 - \alpha_3 \\
  \alpha_2 = \alpha_3
\end{cases}
\quad \Leftrightarrow \quad
\begin{cases}
  \alpha_1 = - 2 \alpha_3 \\
  \alpha_2 = \alpha_3
\end{cases}
\]

So 
\[
  \alpha = 
  \begin{bmatrix}  
      \alpha_1 \\ \alpha_2 \\ \alpha_3
  \end{bmatrix} 
  =
  \begin{bmatrix}  
      -2 \alpha_3 \\ \alpha_3 \\ \alpha_3
  \end{bmatrix} 
  =
  \alpha_3
  \begin{bmatrix}  
      -2 \\ 1 \\ 1
  \end{bmatrix} 
\]

and

\begin{gather*}
L(\alpha_3) = y'y - 2y'X\alpha + \alpha'X'X\alpha = 
  y'y - 2y'X\alpha_3 \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + \alpha_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \alpha_3 \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  = \\
  y'y - 2\alpha_3 y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + \alpha_3^2 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix}
\end{gather*}



\subsection{Part 3}
Estimator of $\alpha_3$ is equal to 
\[ \hat{\alpha}_3 = \text{arg}\min_{\alpha_3\ \ } L(\alpha_3) \]

Again, we will use the formula $\frac{\partial A\beta}{\partial \beta'} = A$ to calculate 
\[ \frac{\partial L(\alpha_3)}{\partial \alpha_3} = 
  0 - 2 y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  + 2 \alpha_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
\]
From F.O.C and the above formula we obtain 
\begin{align*}
  \hat{\alpha}_3 \begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} 
  & = y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \\
  \hat{\alpha}_3 & =
    y'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \left(\begin{bmatrix} -2 & 1 & 1 \end{bmatrix} X'X \begin{bmatrix} -2 \\ 1 \\ 1 \end{bmatrix} \right)^{-1}\\
\end{align*}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 3}



Next, we consider the following model
\[y_{n} = \alpha_{1}X_{1n}+\alpha_{2}X_{2n}+\varepsilon_{n}\]
with $\alpha_{1} = \alpha_{2} = 1$ and $\varepsilon_{n}\sim N(0,1)$.



\subsection{Part 1}
We will generate samples from above model in two cases:
\begin{enumerate}
\item[a)] $X_{1}$ and $X_{2}$ are independent and $X \sim N(0,I_{2})$,
\item[b)] $X_{1}$ and $X_{2}$ are dependent and $X \sim N(0,\Sigma)$.
\end{enumerate}
<<ex3.1, echo=FALSE, eval=TRUE, results='asis'>>=
#task 3
#dot 1, generating model a and b
library(mvtnorm)
n <- 100
#model a
epsilon_a <- rnorm(n, mean = 0, sd = 1)
sigma <- diag(2) 
mean <- c(0,0)
X_a <- rmvnorm(n, mean , sigma)
Y_a <- X_a[,1] + X_a[,2] + epsilon_a
par(mfrow=c(2,1),mar=c(3, 4, 3, 3))
plot(Y_a, main = 'Model a')
#model b
epsilon_b <- rnorm(n, mean = 0, sd = 1)
sigma <- matrix(c(2,0.5,0.5,2),2,2)
X_b <- rmvnorm(n, mean , sigma)
Y_b <- X_b[,1] + X_b[,2] + epsilon_b
plot(Y_b, main = 'Model b')
@



\subsection{Part 2 and 3}
We have the following models: 
\[ y_{n} = \alpha_{1}X_{1n}+\alpha_{2}X_{2n}+\varepsilon_{n} \]
\[ y_{n} = \beta X_{1n}+u_{n} \]
We will use the formula from subsection \ref{referencja} to obtain $\alpha_{1}$ and $\beta$ in those models. 
Below we compare the results for different sample sizes.
<<ex3.2, echo=FALSE, eval=TRUE, results='asis'>>=
#dot 2 and 3
# computing alpha1 and beta estimators for model a and b
mean_a <- c(0,0)
sigma_a <- diag(2) 
mean_b <- c(0,0)
sigma_b <- matrix(c(2,0.5,0.5,2),2,2)
n <- c(10,100,1000)
resalts <- matrix(c(2,0.5,0.5,2),2,2)
alpha_est_a_1 <- integer(3)
beta_est_a <- integer(3)
alpha_est_b_1 <- integer(3)
beta_est_b <- integer(3)
j <- 1
# one loop runs for n = (10,100,1000), second loop is the Monte Carlo loop (1000)
for (i in n){
  alpha_est_a_1_mc <- integer(1000)
  beta_est_a_mc <- integer(1000)
  alpha_est_b_1_mc <- integer(1000)
  beta_est_b_mc <- integer(1000)
  for (k in 1:1000){
  epsilon_a <- rnorm(i, mean = 0, sd = 1)
  X_a <- rmvnorm(i, mean_a , sigma_a)
  Y_a <- X_a[,1] + X_a[,2] + epsilon_a
  alpha_est_a <- solve(t(X_a)%*%X_a)%*%t(X_a)%*%as.numeric(Y_a) 
  alpha_est_a_1_mc[k] <- alpha_est_a[1]
  beta_est_a_mc[k] <- solve(t(X_a[,1])%*%X_a[,1])%*%t(X_a[,1])%*%as.numeric(Y_a)
  epsilon_b <- rnorm(i, mean = 0, sd = 1)
  X_b <- rmvnorm(i, mean_b , sigma_b)
  Y_b <- X_b[,1] + X_b[,2] + epsilon_b
  alpha_est_b <- solve(t(X_b)%*%X_b)%*%t(X_b)%*%as.numeric(Y_b)
  alpha_est_b_1_mc[k] <- alpha_est_b[1]
  beta_est_b_mc[k] <- solve(t(X_b[,1])%*%X_b[,1])%*%t(X_b[,1])%*%as.numeric(Y_b)
  }
  alpha_est_a_1[j] <- mean(alpha_est_a_1_mc) 
  beta_est_a[j] <- mean(beta_est_a_mc)
  alpha_est_b_1[j] <- mean(alpha_est_b_1_mc)
  beta_est_b[j] <- mean(beta_est_b_mc)
  j <- j+1
}

# create results as table
a <- data.frame(N = c(10,100,1000), theoretical_alpha_1 = c(1,1,1), alpha_1_est = alpha_est_a_1, beta_est = beta_est_a)
b <- data.frame(N = c(10,100,1000), theoretical_alpha_1 = c(1,1,1), alpha_1_est = alpha_est_b_1, beta_est = beta_est_b)
table_a <- xtable(a, digits = 5, caption = 'Estimator of alpha1 and beta for model a')
table_b <- xtable(b, digits = 5, caption = 'Estimator of alpha1 and beta for model a')
print(table_a)
print(table_b)
@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
