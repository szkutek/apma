\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings
<<global_settings, echo=FALSE, warning=FALSE>>=
library(knitr)
library(xtable)
library(mvtnorm) 
library(matlib) 
library(plot3D)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=5, fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 2}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exercise 1}


We have data file with 100 rows and 4 columns. We take the first column as a column vector $y$ and the remaining 3 columns as a matrix $X$, where $y$ depends on $X$.
We assume that the model for our data is as follows
\begin{equation} 
\label{eq1}
y = X \alpha + u. 
\end{equation}

We will use regression model function in R to compute the parameters and compare them with the results we obtain manually. 
<<ex1summary, echo=TRUE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
attach(data)

N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
# linear regression model using lm()
model <- lm(V1 ~ . - 1, data)

summary(model)
@

\subsection{Part 1}
We want to express the loss function 
\[ L = \sum_{n=1}^{N} u_n^2 \]
as a function of $y$, $X$ and $\alpha$. From \eqref{eq1} we have $u=y-X\alpha$. Then

\begin{gather*} 
L = \sum_{n=1}^{N} u_n^2 = u' u = (y-X\alpha)' (y-X\alpha) = (y'-\alpha' X')(y-X\alpha) = \\
= y'y - y'X\alpha - \alpha'X'y + \alpha'X'X\alpha = y'y - 2y'X\alpha + \alpha'X'X\alpha.
\end{gather*}


\subsection{Part 2}
Next we will use the following equalities
\[ \frac{\partial A\beta}{\partial \beta'} = A \qquad \text{and} \qquad  \frac{\partial \beta' A\beta}{\partial \beta'} = \beta'(A+A')\]
to calculate the first derivative of $L$ with respect to $\alpha$.

\[ \frac{\partial L(\alpha)}{\partial \alpha'} = 0 - 2y'X + \alpha'(X'X + X'X) = - 2y'X +2 \alpha'X'X. \]


\subsection{Part 3}
Now, to minimize the $L$ function, we will solve the first order condition equation $\frac{\partial L(\alpha)}{\partial \alpha'} = 0 $.

\begin{align*}
\frac{\partial L(\alpha)}{\partial \alpha'} & = 0 \\
-2 y'X + 2 \alpha'X'X  & = 0 \\
\alpha'X'X & = y'X \ \ /\cdot (X'X)^{-1} \\
\alpha' & = y'X (X'X)^{-1} \\
\alpha & = (X'X)^{-1} X'y
\end{align*}
So $\hat{\alpha}  = (X'X)^{-1} X'y$ is the LS estimator of the model parameter.

The first vector is the theoretical estimator of $\alpha$ and the second is the estimator obtained with R's linear regression model:
<<ex1.alpha.est, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

# theoretical result:
alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y
# r model
model <- lm(V1 ~ . - 1, data)
alpha.model.est <- summary(model)$coefficients[, 1]
data.names <- c("alpha_1","alpha_2","alpha_3")
mat1 <- matrix(c(data.names, alpha.est, alpha.model.est), nrow=length(alpha.est))
tab1 <- xtable(mat1, digits = 5, row.names = FALSE, 
               caption = "Estimator of alpha", label = "tab:alpha.est")
names(tab1) <- c(" ", "theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@



\subsection{Part 4}
We are using unbiased estimator for the variance of residuals
\[ \hat{\sigma}^2 = \frac{u'u}{N-K},\]
where in our case $N=100$ and $K=3$. 
<<ex1.resid.var, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals
# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)
# summary(model) # Residual standard error squared

tab1 <- xtable(matrix(c(sigma2.est.unbiased, 0.8956 ^ 2), 1, 2), 
               digits = 5, row.names = FALSE, 
               caption = "Variance of residuals", label = "tab:table1")
names(tab1) <- c("theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@
The first row is the theoretical estimator of $\sigma$ and the second is the squared residual standard error obtained with R's linear regression model.



\subsection{Part 5}

<<ex1.resid.white.noise, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

plot(res)
@

We assume that the residuals are uncorrelated and homoscedastic. The variance-covariance matrix of LS estimator is
\[ \hat{\Sigma}_{\hat{\alpha}} = \hat{\sigma}^2 (X'X)^{-1}, \]
where $\hat{\sigma}^2$ is the variance of the residuals.

We can calculate the variance-covariance matrix using R:
<<ex1.resid.matrix, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y

model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)

# VARIANCE-COVARIANCE MATRIX OF LS ESTIMATOR
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators
cov.alpha.est
vcov(model)
@
where the first result is the theoretical estimator of $\Sigma$ and the second is the one obtained with R's linear regression model.

Variance-covariance matrix for $\sqrt{N}\hat{\alpha}$ is equal to:
\[ \hat{\Sigma}_{\sqrt{N}\hat{\alpha}} = N\hat{\sigma}^2 (X'X)^{-1}\]
<<ex1.resid.matrix.N, echo=FALSE, eval=TRUE>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])

alpha.est <- solve(t(X) %*% X) %*% t(X) %*% y

model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals

# VARIANCE OF RESIDUALS
sigma2.est.unbiased <- t(res) %*% res / (N - K)

# VARIANCE-COVARIANCE MATRIX OF LS ESTIMATOR
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators
cov.alpha.est.N <- N * cov.alpha.est
cov.alpha.est.N
@



\subsection{Part 6}
The $t$-statistic tests the hypothesis $H_0:\ \alpha_i = 0$, $H_1:\ \alpha_i \neq 0$. The $t$-ratio is the ratio of the sample regression coefficient to its standard error. So
\[ t_{\hat{\alpha_i}} = \frac{\hat{\alpha_i}}{\sqrt{Var\hat{\alpha_i}}}, \]
where $t_{\hat{\alpha_i}} \sim t(N-K) = t(100-3) = t(97)$.

<<ex1.t.ratios, echo=FALSE, eval=TRUE, results='asis'>>=
data <- read.table('data_lab_2.csv', sep = ",", dec = ",", header = FALSE)
N <- 100
K <- 3
X <- as.matrix(data[, -1])
y <- as.matrix(data[, 1])
model <- lm(V1 ~ . - 1, data)
res <- resid(model) # residuals
sigma2.est.unbiased <- t(res) %*% res / (N - K) # variance of residuals
cov.alpha.est <- solve(t(X) %*% X) * as.numeric(sigma2.est.unbiased) # matrix of estimators

se.alpha.est <- sqrt(diag(cov.alpha.est))
t.ratio.theoretical <- alpha.est / se.alpha.est
t.ratio.empirical <- summary(model)$coefficients[, 3]


mat <- matrix(c("V1", "V2", "V3", t.ratio.theoretical, t.ratio.empirical), nrow=length(t.ratio.theoretical)) 
tab1 <- xtable(mat, digits = 3, row.names = FALSE,
               caption = "t-ratios of the parameters", label = "tab:table2")
names(tab1) <- c(" ", "theoretical", "using lm()")
print(tab1, type = "latex", table.placement = "H")
@






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 2}

In this exercise we assume that $\alpha_1 + \alpha_2 + \alpha_3 = 0$ and $\alpha_2 - \alpha_3 = 0$. 

\subsection{Part 1}
We know that the restriction matrix $R$ satisfies equation $R \alpha = r$. In this case 
\[ 
  R \cdot
  \begin{bmatrix}  
      \alpha_1 \\ \alpha_2 \\ \alpha_3
  \end{bmatrix} 
  =
  \begin{bmatrix}  
    0 \\ 0
  \end{bmatrix} 
  \quad \Rightarrow \quad
  R = 
  \begin{bmatrix}  
    1 & 1 & 1 \\
    0 & 1 & -1
  \end{bmatrix}
\]
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Exercise 3}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
