\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings
<<global_settings, echo=FALSE, warning=FALSE>>=
library(knitr)
library(xtable)
library(mvtnorm) 
library(matlib) 
library(plot3D)
library(readxl)
opts_chunk$set(fig.path='figure/', fig.align='center', fig.pos='H',fig.width=7, fig.height=4)
@

\newtheorem{lemma}{Lemma}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 5}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<ex1.2, echo=FALSE, eval=FALSE, fig.cap="">>=
@


<<ex2.4, echo=FALSE, eval=FALSE>>=
@


\section{Exercise 1}

We consider data from file \textit{data-Lab5.xlsx}, which consists of two variables $X$ and $Y$. We assume that $Y$ is generated by the process 
\[ y_i = \alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + e_i,  \]
where $e_i$ is white noise and $e_i \sim N(0,\sigma^2)$.

\subsection{Part 1}
We will verify hypothesis $H_0:\ \sigma^2=1$ versus the alternative $H_1:\ \sigma^2>1$ with three tests.

The restriction function is $h(\theta)=R\theta-r$, where $\theta = [\alpha_0,\alpha_1,\alpha_2,\sigma^2]'$, $R= [0,0,0,1]$ and $r=1$. 
Additionally $H(\theta)=h'(\theta)=R$.

From W. Green \textit{Econometric analysis}, for independent $x_1,\dots, x_p$ and $y = X \beta + \varepsilon$, where $ \varepsilon_i \sim N(0,\sigma^2)$ we get the following ML estimators
\[ \hat{\beta}_{ML} = (X'X)^{-1} X' y \qquad\text{and}\qquad \hat{\sigma}^2_{ML} = \frac{\hat{e}' \hat{e}}{N},\]
where $\hat{e} = y-X\hat{\beta}$ is estimator of residuals.
The log-likelihood function is equal to 
\[ \ln{L(\theta)} = -\frac{N}{2}\ln{2\pi} - \frac{N}{2}\ln{\sigma^2} - \frac{(y-X\beta)'(y-X\beta)}{2\sigma^2}\]
and the inverse information matrix is as follows
\[ I^{-1}(\hat{\theta}) =
 \begin{bmatrix}
  \sigma^2(X'X)^{-1} & 0 \\
  0' & \frac{2\sigma^4}{N}
 \end{bmatrix},
 \quad\text{where}\qquad
 \theta = [\beta, \sigma^2]'.
\]
In our case $\beta = [\alpha_0,\alpha_1,\alpha_2]'$.

\subsubsection{Wald test}
In general 
\[ W = h^T(\hat{\theta}) \left( H(\hat{\theta}) I^{-1}(\hat{\theta}) H^T(\hat{\theta}) \right)^{-1} h(\hat{\theta}) \]
and in our case
\[ W = [R \hat{\theta} - 1]' \left[ R\ I^{-1}(\hat{\theta})\ R^T \right]^{-1} [R \hat{\theta} -1] \rightarrow^d \chi^2(1) \]
<<ex1.wald, echo=FALSE, eval=TRUE>>=
library(readxl)
data <- read_excel('data_Lab5.xlsx', col_names = F)
data <- as.matrix(data)
N <- nrow(data)
y <- data[, 1]
x <- data[, 2]
X <- cbind(rep(1, N), x, x ^ 2)

# MLE
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
sigma2.mle <- t(y - X %*% beta.mle) %*% (y - X %*% beta.mle) / N

theta.hat <- c(beta.mle, sigma2.mle)
sigma2.mle <- as.numeric(sigma2.mle)
I.inv <- rbind(cbind(sigma2.mle * solve(t(X) %*% X), rep(0, 3)),
               cbind(t(rep(0, 3)), 2 * sigma2.mle ^ 2 / N))

# Wald test
R <- matrix(c(0, 0, 0, 1), 1, 4)
W <-
  t(R %*% theta.hat - 1) %*% solve(R %*% I.inv %*% t(R)) %*% (R %*% theta.hat - 1)

alpha <- 0.05
cat('alpha =', alpha)
W.pval <- 1 - pchisq(1 - alpha, df = 1) # 0.3297193
cat('p-value =', W.pval)
critical.val <- qchisq(1 - alpha, df = 1) # 3.841459
cat('Critical value =', critical.val)
cat('Wald test statistic =', W)
@
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|W|<$ \textit{critical value}.



\subsubsection{Likelihood ratio test}
In general 
\[ LR = 2\left( \ln{L(\hat{\theta})} - \ln{L(\hat{\theta}_R)} \right), \]
where $\hat{\theta}_R$ is the restricted estimator of $\theta$ and is equal to $\hat{\theta}_R = \left [\hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2, 1 \right]$. In our case
\[ LR = 2\left( -\frac{N}{2}\ln{2\pi} - \frac{N}{2}\ln{\hat{\sigma}^2} - \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2\hat{\sigma}^2}
                +\frac{N}{2}\ln{2\pi} + \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2}
              \right) \]

<<ex1.lr, echo=FALSE, eval=TRUE>>=
library(readxl)
data <- read_excel('data_Lab5.xlsx', col_names = F)
data <- as.matrix(data)
N <- nrow(data)
y <- data[, 1]
x <- data[, 2]
X <- cbind(rep(1, N), x, x^2)

# MLE
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
sigma2.mle <- t(y - X %*% beta.mle) %*% (y - X %*% beta.mle) / N
sigma2.mle <- as.numeric(sigma2.mle)

ln_L_theta_hat = -N*log(2*pi)/2-N*log(sigma2.mle)/2 -t(y-X%*%beta.mle)%*%(y-X%*%beta.mle)/(2*sigma2.mle)
ln_L_theta_hat_R = -N*log(2*pi)/2-N*log(1)/2-t(y-X%*%beta.mle)%*%(y-X%*%beta.mle)/(2*1)

# Likelihood ratio test
LR = 2*(ln_L_theta_hat - ln_L_theta_hat_R)

alpha <- 0.05
cat('alpha =', alpha)
LR.pval <- 1 - pchisq(1 - alpha, df = 1) # 0.3297193
cat('p-value =', LR.pval)
critical.val <- qchisq(1 - alpha, df = 1) # 3.841459
cat('Critical value =', critical.val)
cat('LR test statistic =', LR)
@
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|LR|<$ \textit{critical value}.

\subsubsection{Lagrange multiplier test}
In general 
\[ LM = Dl(\hat{\theta}_R)' I^{-1}(\hat{\theta}_R) Dl(\hat{\theta}_R).\]
In our case
\[ Dl(\hat{\theta}_R) = 
  \begin{bmatrix} 
    X'(y-X\hat{\beta}) \\
    -\frac{N}{2} + \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2}
  \end{bmatrix}
\]
and 
\[ I^{-1}(\hat{\theta}_R) =
 \begin{bmatrix}
  (X'X)^{-1} & 0 \\
  0' & \frac{2}{N}
 \end{bmatrix}.
\]

<<ex1.lm, echo=FALSE, eval=TRUE>>=
library(readxl)
data <- read_excel('data_Lab5.xlsx', col_names = F)
data <- as.matrix(data)
N <- nrow(data)
y <- data[, 1]
x <- data[, 2]
X <- cbind(rep(1, N), x, x^2)

# MLE
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
sigma2.mle <- t(y - X %*% beta.mle) %*% (y - X %*% beta.mle) / N

sigma2.mle <- as.numeric(sigma2.mle)

Dl_theta_R = rbind(t(X)%*%(y-X%*%beta.mle),-N/2 + t(y-X%*%beta.mle)%*%(y-X%*%beta.mle)/2)
I_inv_theta_R = rbind(cbind(solve(t(X) %*% X), rep(0, 3)),
                      cbind(t(rep(0, 3)), 2/ N))


# Lagrange multiplier test
LM = t(Dl_theta_R)%*%I_inv_theta_R%*%Dl_theta_R

alpha <- 0.05
cat('alpha =', alpha)
LM.pval <- 1 - pchisq(1 - alpha, df = 1) # 0.3297193
cat('p-value =', LM.pval)
critical.val <- qchisq(1 - alpha, df = 1) # 3.841459
cat('Critical value =', critical.val)
cat('LM test statistic =', LM)
@
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|LM|<$ \textit{critical value}.


\subsection{Part 2}
We want to test if the polynomial $\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 $ has exactly one root. We know that it has only one root if and only if 
$ \alpha_1^2 - 4\alpha_0 \alpha_2 = 0$.
So
\[ H_0:\ h(\theta) = \alpha_1^2 - 4\alpha_0 \alpha_2 = 0. \]
Then 
\[ H(\theta) = h'(\theta) = [-4\alpha_2,\ 2\alpha_1,\ -4\alpha_0,\ 0]. \]
The Wald test statistic is equal to
\[ W = h^T(\hat{\theta}) \left( H(\hat{\theta}) I^{-1}(\hat{\theta}) H^T(\hat{\theta}) \right)^{-1} h(\hat{\theta}). \]
In our case it is equal to
\[ W = \frac{1}{\hat{\sigma}^2} \frac{(\alpha_1^2 - 4\alpha_0 \alpha_2)^2}
      { [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0] (X'X)^{-1} [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0]' } \rightarrow \chi^2(1). \]
Asymptotic distribution of the parameters
\[ \sqrt{N} h(\hat{\theta}) \rightarrow N\left(0, H(\theta_0)  i^{-1}(\theta_0) H^T(\theta_0) \right) \]
% In our case
% \[ \sqrt{N} h(\hat{\theta}) \rightarrow N\left(0, 
%     \hat{\sigma}^2 [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0] (X'X)^{-1} [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0]' 
%     \right) \]
Other test statistics are equal to
\[ LR = 2\left( \ln{L(\hat{\theta})} - \ln{L(\hat{\theta}_R)} \right) \quad\text{and}\quad LM = Dl(\hat{\theta}_R)' I^{-1}(\hat{\theta}_R) Dl(\hat{\theta}_R), \]
where 
\[\hat{\theta}_R = \left [\frac{\hat{\alpha}_1^2}{4\hat{\alpha}_2}, \hat{\alpha}_1, \hat{\alpha}_2, \hat{\sigma}^2 \right]. \]


\subsection{Part 3}
We use Wald test statistic to verify the null hypothesis from the previous task. The results are as follows
<<ex1.part3, echo=FALSE, eval=TRUE>>=
library(readxl)
data <- read_excel('data_Lab5.xlsx', col_names = F)
data <- as.matrix(data)
N <- nrow(data)
y <- data[, 1]
x <- data[, 2]
X <- cbind(rep(1, N), x, x^2)

# MLE
beta.mle <- solve(t(X) %*% X) %*% t(X) %*% y
sigma2.mle <- t(y - X %*% beta.mle) %*% (y - X %*% beta.mle) / N

theta.hat <- c(beta.mle, sigma2.mle)
sigma2.mle <- as.numeric(sigma2.mle)
I.inv <- rbind(cbind(sigma2.mle * solve(t(X) %*% X), rep(0, 3)),
               cbind(t(rep(0, 3)), 2 * sigma2.mle ^ 2 / N))

# Wald test
alpha_0_hat <- beta.mle[1]
alpha_1_hat <- beta.mle[2]
alpha_2_hat <- beta.mle[3]
wector <- matrix(c(-4*alpha_2_hat,2*alpha_1_hat,-4*alpha_0_hat),1,3)

W = ((alpha_1_hat^2-4*alpha_0_hat*alpha_2_hat)^2)/(sigma2.mle*wector%*%solve(t(X)%*%X)%*%t(wector))

alpha <- 0.05
cat('alpha =', alpha)
W.pval <- 1 - pchisq(1 - alpha, df = 1) # 0.3297193
cat('p-value =', W.pval)
critical.val <- qchisq(1 - alpha, df = 1) # 3.841459
cat('Critical value =', critical.val)
cat('Wald test statistic =', W)
@
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|W|<$ \textit{critical value}.




\section{Exercise 2}
We want to verify if in the model 
\[ y_t = \alpha x_t + e_t \]
the parameter $\alpha$ is different than zero. We will test the hypothesis with a $t$-Student test. In order to verify the test performance we will conduct a small Monte Carlo experiment.

We define number of MC iterations $N=1000$ and the sample size $T=200$. We will compute the frequency of rejections for $\alpha = 2$ and $\alpha_0 \in [1.8,2.2]$. 

We generate $N$ different realizations of the stationary exogenous variable $x_t$ from normal distribution $N(5,1)$.

The $t$-statistic is as follows
\[ 
  \frac{|\hat{\alpha} - \alpha_0|}{\text{SE}_{\hat{\alpha}}} \sim t(T-2)
  \quad\text{and}\quad
  \text{SE}_{\hat{\alpha}} = 
    \frac{\sqrt{ \frac{1}{T-2}  \sum_{i=1}^T (y_i - \hat{y}_i)^2}}
    {\sqrt{\sum_{i=1}^T (x_i - \bar{x})^2}},
\]
where
\[
  y_i - \hat{y}_i = y_i - \hat{\alpha}x_i
  \quad\text{and}\quad
  \hat{\alpha} = (X'X)^{-1} X' Y.
\]


\subsection{Test properties}
To test properties for stationary residuals for each MC iteration we generate white noise $e_t\sim N(0,\sigma^2)$, $\sigma=1$. Then with $\alpha=2$ we generate $y_t$ and compute $t$-statistic and frequency of rejections using the following functions.
<<ex2.functions, echo=TRUE, eval=TRUE>>=
Monte_Carlo_test_normal <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- rnorm(200,mean = 5, sd = 1)
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}

Monte_Carlo_test_random_walk <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- cumsum(rnorm(200,mean = 5, sd = 1))
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}
@


We get the following results 
<<ex2.freq1, echo=FALSE, eval=TRUE, fig.cap="Frequency of null rejection with residuals from N(0,1)">>=
Monte_Carlo_test_normal <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- rnorm(200,mean = 5, sd = 1)
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}
N <- 1000
T <- 200
# residua from normal distribution
alpha <- 2
alpha0 <- seq(alpha-0.2,alpha+0.2,0.01)
res <- rep(0,length(alpha0))
j <- 1
for (i in alpha0)
{
  res[j] <- Monte_Carlo_test_normal(i,alpha,N,T)
  j = j+1
}
plot(alpha0,res)
@


<<ex2.freq2, echo=FALSE, eval=TRUE, fig.cap="Frequency of null rejection with residuals from random walk">>=
Monte_Carlo_test_random_walk <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- cumsum(rnorm(200,mean = 5, sd = 1))
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}
N <- 1000
T <- 200
# residua from random walk
alpha <- 2
alpha0 <- seq(alpha-0.2,alpha+0.2,0.01)
res <- rep(0,length(alpha0))
j <- 1
for (i in alpha0)
{
  res[j] <- Monte_Carlo_test_random_walk(i,alpha,N,T)
  j = j+1
}
plot(alpha0,res)
@

In both cases the size of the test is close to 0
<<ex2.size, echo=FALSE, eval=TRUE>>=
N <- 1000
T <- 200

Monte_Carlo_test_normal <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- rnorm(200,mean = 5, sd = 1)
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}

Monte_Carlo_test_random_walk <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- cumsum(rnorm(200,mean = 5, sd = 1))
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}

# size
cat("Size for residuals from N(0,1): ", Monte_Carlo_test_normal(2,2,N,T))
cat("Size for residuals from random walk: ", Monte_Carlo_test_random_walk(2,2,N,T))
@

And the powers are as follows 
<<ex2.power2, echo=FALSE, eval=TRUE, fig.cap="Power of the test with residuals from N(0,1)">>=
N <- 1000
T <- 200

Monte_Carlo_test_normal <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- rnorm(200,mean = 5, sd = 1)
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}
# power
# residua from normal distribution
alpha0 <- c(seq(-1,-0.1,0.05),seq(0.1,1,0.05))
power <- rep(0,length(alpha0))
j <- 1
for (i in alpha0)
{
  power[j] <- Monte_Carlo_test_normal(i,0,N,T)
  j = j+1
}
plot(alpha0,power)
@

<<ex2.power1, echo=FALSE, eval=TRUE, fig.cap="Power of the test with residuals from random walk">>=
N <- 1000
T <- 200
Monte_Carlo_test_random_walk <- function(alpha0,alpha,N,T)
{
  a <- 0.05
  critical.val <- qt(1 - a, df = T-2)
  rejecting = 0
  for (i in 1:N)
  {
    x <- cumsum(rnorm(200,mean = 5, sd = 1))
    e <- rnorm(200,mean = 0, sd = 1)
    y <- alpha*x + e
    X <- as.matrix(x)
    Y <- as.matrix(y)
    alpha_est = solve(t(X)%*%X)%*%t(X)%*%Y
    SE_alpha_est = sqrt(sum((y-alpha_est*x)^2)/(T-2))/sqrt(sum((x-mean(x))^2))
    t_student_test = abs(alpha_est - alpha0)/SE_alpha_est
    if (abs(t_student_test)>=critical.val)
    {
      rejecting = rejecting + 1
    }
  }
  return(rejecting/N)
}
# power
# residua from random walk
alpha0 <- c(seq(-1,-0.1,0.05),seq(0.1,1,0.05))
power <- rep(0,length(alpha0))
j <- 1
for (i in alpha0)
{
  power[j] <- Monte_Carlo_test_random_walk(i,0,N,T)
  j = j+1
}
plot(alpha0,power)
@

In the case of random walk even if $H_0$ is very close to the true value, the test will reject it.

\end{document}
