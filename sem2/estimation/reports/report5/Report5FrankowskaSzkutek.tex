\documentclass[12pt, a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX packages
%\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% global settings


\newtheorem{lemma}{Lemma}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% title page
\title{Estimation theory -- Report 5}
\author{Marta Frankowska, 208581 \\ Agnieszka Szkutek, 208619}
\maketitle
\tableofcontents 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\section{Exercise 1}

We consider data from file \textit{data-Lab5.xlsx}, which consists of two variables $X$ and $Y$. We assume that $Y$ is generated by the process 
\[ y_i = \alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 + e_i,  \]
where $e_i$ is white noise and $e_i \sim N(0,\sigma^2)$.

\subsection{Part 1}
We will verify hypothesis $H_0:\ \sigma^2=1$ versus the alternative $H_1:\ \sigma^2>1$ with three tests.

The restriction function is $h(\theta)=R\theta-r$, where $\theta = [\alpha_0,\alpha_1,\alpha_2,\sigma^2]'$, $R= [0,0,0,1]$ and $r=1$. 
Additionally $H(\theta)=h'(\theta)=R$.

From W. Green \textit{Econometric analysis}, for independent $x_1,\dots, x_p$ and $y = X \beta + \varepsilon$, where $ \varepsilon_i \sim N(0,\sigma^2)$ we get the following ML estimators
\[ \hat{\beta}_{ML} = (X'X)^{-1} X' y \qquad\text{and}\qquad \hat{\sigma}^2_{ML} = \frac{\hat{e}' \hat{e}}{N},\]
where $\hat{e} = y-X\hat{\beta}$ is estimator of residuals.
The log-likelihood function is equal to 
\[ \ln{L(\theta)} = -\frac{N}{2}\ln{2\pi} - \frac{N}{2}\ln{\sigma^2} - \frac{(y-X\beta)'(y-X\beta)}{2\sigma^2}\]
and the inverse information matrix is as follows
\[ I^{-1}(\hat{\theta}) =
 \begin{bmatrix}
  \sigma^2(X'X)^{-1} & 0 \\
  0' & \frac{2\sigma^4}{N}
 \end{bmatrix},
 \quad\text{where}\qquad
 \theta = [\beta, \sigma^2]'.
\]
In our case $\beta = [\alpha_0,\alpha_1,\alpha_2]'$.

\subsubsection{Wald test}
In general 
\[ W = h^T(\hat{\theta}) \left( H(\hat{\theta}) I^{-1}(\hat{\theta}) H^T(\hat{\theta}) \right)^{-1} h(\hat{\theta}) \]
and in our case
\[ W = [R \hat{\theta} - 1]' \left[ R\ I^{-1}(\hat{\theta})\ R^T \right]^{-1} [R \hat{\theta} -1] \rightarrow^d \chi^2(1) \]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## alpha = 0.05
## p-value = 0.3297193
## Critical value = 3.841459
## Wald test statistic = 0.4598706
\end{verbatim}
\end{kframe}
\end{knitrout}
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|W|<$ \textit{critical value}.



\subsubsection{Likelihood ratio test}
In general 
\[ LR = 2\left( \ln{L(\hat{\theta})} - \ln{L(\hat{\theta}_R)} \right), \]
where $\hat{\theta}_R$ is the restricted estimator of $\theta$ and is equal to $\hat{\theta}_R = \left [\hat{\alpha}_0, \hat{\alpha}_1, \hat{\alpha}_2, 1 \right]$. In our case
\[ LR = 2\left( -\frac{N}{2}\ln{2\pi} - \frac{N}{2}\ln{\hat{\sigma}^2} - \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2\hat{\sigma}^2}
                +\frac{N}{2}\ln{2\pi} + \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2}
              \right) \]

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## alpha = 0.05
## p-value = 0.3297193
## Critical value = 3.841459
## LR test statistic = 0.4418897
\end{verbatim}
\end{kframe}
\end{knitrout}
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|LR|<$ \textit{critical value}.

\subsubsection{Lagrange multiplier test}
In general 
\[ LM = Dl(\hat{\theta}_R)' I^{-1}(\hat{\theta}_R) Dl(\hat{\theta}_R).\]
In our case
\[ Dl(\hat{\theta}_R) = 
  \begin{bmatrix} 
    X'(y-X\hat{\beta}) \\
    -\frac{N}{2} + \frac{(y-X\hat{\beta})'(y-X\hat{\beta})}{2}
  \end{bmatrix}
\]
and 
\[ I^{-1}(\hat{\theta}_R) =
 \begin{bmatrix}
  (X'X)^{-1} & 0 \\
  0' & \frac{2}{N}
 \end{bmatrix}.
\]

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## alpha = 0.05
## p-value = 0.3297193
## Critical value = 3.841459
## LM test statistic = 0.4331969
\end{verbatim}
\end{kframe}
\end{knitrout}
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|LM|<$ \textit{critical value}.


\subsection{Part 2}
We want to test if the polynomial $\alpha_0 + \alpha_1 x_i + \alpha_2 x_i^2 $ has exactly one root. We know that it has only one root if and only if 
$ \alpha_1^2 - 4\alpha_0 \alpha_2 = 0$.
So
\[ H_0:\ h(\theta) = \alpha_1^2 - 4\alpha_0 \alpha_2 = 0. \]
Then 
\[ H(\theta) = h'(\theta) = [-4\alpha_2,\ 2\alpha_1,\ -4\alpha_0,\ 0]. \]
The Wald test statistic is equal to
\[ W = h^T(\hat{\theta}) \left( H(\hat{\theta}) I^{-1}(\hat{\theta}) H^T(\hat{\theta}) \right)^{-1} h(\hat{\theta}). \]
In our case it is equal to
\[ W = \frac{1}{\hat{\sigma}^2} \frac{(\alpha_1^2 - 4\alpha_0 \alpha_2)^2}
      { [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0] (X'X)^{-1} [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0]' } \rightarrow \chi^2(1). \]
Asymptotic distribution of the parameters
\[ \sqrt{N} h(\hat{\theta}) \rightarrow N\left(0, H(\theta_0)  i^{-1}(\theta_0) H^T(\theta_0) \right) \]
% In our case
% \[ \sqrt{N} h(\hat{\theta}) \rightarrow N\left(0, 
%     \hat{\sigma}^2 [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0] (X'X)^{-1} [-4\alpha_2, 2\alpha_1, -4\alpha_0, 0]' 
%     \right) \]
Other test statistics are equal to
\[ LR = 2\left( \ln{L(\hat{\theta})} - \ln{L(\hat{\theta}_R)} \right) \quad\text{and}\quad LM = Dl(\hat{\theta}_R)' I^{-1}(\hat{\theta}_R) Dl(\hat{\theta}_R), \]
where 
\[\hat{\theta}_R = \left [\frac{\hat{\alpha}_1^2}{4\hat{\alpha}_2}, \hat{\alpha}_1, \hat{\alpha}_2, \hat{\sigma}^2 \right]. \]


\subsection{Part 3}
We use Wald test statistic to verify the null hypothesis from the previous task. The results are as follows
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## alpha = 0.05
## p-value = 0.3297193
## Critical value = 3.841459
## Wald test statistic = 1.422491
\end{verbatim}
\end{kframe}
\end{knitrout}
We can't reject the null hypothesis, because \textit{p-value} $> \alpha$ and $|W|<$ \textit{critical value}.




\section{Exercise 2}
We want to verify if in the model 
\[ y_t = \alpha x_t + e_t \]
the parameter $\alpha$ is different than zero. We will test the hypothesis with a $t$-Student test. In order to verify the test performance we will conduct a small Monte Carlo experiment.

We define number of MC iterations $N=1000$ and the sample size $T=200$. We will compute the frequency of rejections for $\alpha = 2$ and $\alpha_0 \in [1.8,2.2]$. 

We generate $N$ different realizations of the stationary exogenous variable $x_t$ from normal distribution $N(5,1)$.

The $t$-statistic is as follows
\[ 
  \frac{|\hat{\alpha} - \alpha_0|}{\text{SE}_{\hat{\alpha}}} \sim t(T-2)
  \quad\text{and}\quad
  \text{SE}_{\hat{\alpha}} = 
    \frac{\sqrt{ \frac{1}{T-2}  \sum_{i=1}^T (y_i - \hat{y}_i)^2}}
    {\sqrt{\sum_{i=1}^T (x_i - \bar{x})^2}},
\]
where
\[
  y_i - \hat{y}_i = y_i - \hat{\alpha}x_i
  \quad\text{and}\quad
  \hat{\alpha} = (X'X)^{-1} X' Y.
\]


\subsection{Test properties}
To test properties for stationary residuals for each MC iteration we generate white noise $e_t\sim N(0,\sigma^2)$, $\sigma=1$. Then with $\alpha=2$ we generate $y_t$ and compute $t$-statistic and frequency of rejections using the following functions.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{Monte_Carlo_test_normal} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{alpha0}\hlstd{,}\hlkwc{alpha}\hlstd{,}\hlkwc{N}\hlstd{,}\hlkwc{T}\hlstd{)}
\hlstd{\{}
  \hlstd{a} \hlkwb{<-} \hlnum{0.05}
  \hlstd{critical.val} \hlkwb{<-} \hlkwd{qt}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{a,} \hlkwc{df} \hlstd{= T}\hlopt{-}\hlnum{2}\hlstd{)}
  \hlstd{rejecting} \hlkwb{=} \hlnum{0}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{N)}
  \hlstd{\{}
    \hlstd{x} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{200}\hlstd{,}\hlkwc{mean} \hlstd{=} \hlnum{5}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
    \hlstd{e} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{200}\hlstd{,}\hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
    \hlstd{y} \hlkwb{<-} \hlstd{alpha}\hlopt{*}\hlstd{x} \hlopt{+} \hlstd{e}
    \hlstd{X} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(x)}
    \hlstd{Y} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(y)}
    \hlstd{alpha_est} \hlkwb{=} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)}\hlopt{%*%}\hlstd{X)}\hlopt{%*%}\hlkwd{t}\hlstd{(X)}\hlopt{%*%}\hlstd{Y}
    \hlstd{SE_alpha_est} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{((y}\hlopt{-}\hlstd{alpha_est}\hlopt{*}\hlstd{x)}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{(T}\hlopt{-}\hlnum{2}\hlstd{))}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{((x}\hlopt{-}\hlkwd{mean}\hlstd{(x))}\hlopt{^}\hlnum{2}\hlstd{))}
    \hlstd{t_student_test} \hlkwb{=} \hlkwd{abs}\hlstd{(alpha_est} \hlopt{-} \hlstd{alpha0)}\hlopt{/}\hlstd{SE_alpha_est}
    \hlkwa{if} \hlstd{(}\hlkwd{abs}\hlstd{(t_student_test)}\hlopt{>=}\hlstd{critical.val)}
    \hlstd{\{}
      \hlstd{rejecting} \hlkwb{=} \hlstd{rejecting} \hlopt{+} \hlnum{1}
    \hlstd{\}}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(rejecting}\hlopt{/}\hlstd{N)}
\hlstd{\}}

\hlstd{Monte_Carlo_test_random_walk} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{alpha0}\hlstd{,}\hlkwc{alpha}\hlstd{,}\hlkwc{N}\hlstd{,}\hlkwc{T}\hlstd{)}
\hlstd{\{}
  \hlstd{a} \hlkwb{<-} \hlnum{0.05}
  \hlstd{critical.val} \hlkwb{<-} \hlkwd{qt}\hlstd{(}\hlnum{1} \hlopt{-} \hlstd{a,} \hlkwc{df} \hlstd{= T}\hlopt{-}\hlnum{2}\hlstd{)}
  \hlstd{rejecting} \hlkwb{=} \hlnum{0}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{N)}
  \hlstd{\{}
    \hlstd{x} \hlkwb{<-} \hlkwd{cumsum}\hlstd{(}\hlkwd{rnorm}\hlstd{(}\hlnum{200}\hlstd{,}\hlkwc{mean} \hlstd{=} \hlnum{5}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{))}
    \hlstd{e} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{200}\hlstd{,}\hlkwc{mean} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{1}\hlstd{)}
    \hlstd{y} \hlkwb{<-} \hlstd{alpha}\hlopt{*}\hlstd{x} \hlopt{+} \hlstd{e}
    \hlstd{X} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(x)}
    \hlstd{Y} \hlkwb{<-} \hlkwd{as.matrix}\hlstd{(y)}
    \hlstd{alpha_est} \hlkwb{=} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)}\hlopt{%*%}\hlstd{X)}\hlopt{%*%}\hlkwd{t}\hlstd{(X)}\hlopt{%*%}\hlstd{Y}
    \hlstd{SE_alpha_est} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{((y}\hlopt{-}\hlstd{alpha_est}\hlopt{*}\hlstd{x)}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{(T}\hlopt{-}\hlnum{2}\hlstd{))}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{((x}\hlopt{-}\hlkwd{mean}\hlstd{(x))}\hlopt{^}\hlnum{2}\hlstd{))}
    \hlstd{t_student_test} \hlkwb{=} \hlkwd{abs}\hlstd{(alpha_est} \hlopt{-} \hlstd{alpha0)}\hlopt{/}\hlstd{SE_alpha_est}
    \hlkwa{if} \hlstd{(}\hlkwd{abs}\hlstd{(t_student_test)}\hlopt{>=}\hlstd{critical.val)}
    \hlstd{\{}
      \hlstd{rejecting} \hlkwb{=} \hlstd{rejecting} \hlopt{+} \hlnum{1}
    \hlstd{\}}
  \hlstd{\}}
  \hlkwd{return}\hlstd{(rejecting}\hlopt{/}\hlstd{N)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}


We get the following results 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_freq1-1} 

}

\caption[Frequency of null rejection with residuals from N(0,1)]{Frequency of null rejection with residuals from N(0,1)}\label{fig:ex2.freq1}
\end{figure}


\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_freq2-1} 

}

\caption[Frequency of null rejection with residuals from random walk]{Frequency of null rejection with residuals from random walk}\label{fig:ex2.freq2}
\end{figure}


\end{knitrout}

In both cases the size of the test is close to 0
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## Size for residuals from N(0,1):  0
## Size for residuals from random walk:  0
\end{verbatim}
\end{kframe}
\end{knitrout}

And the powers are as follows 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_power2-1} 

}

\caption[Power of the test with residuals from N(0,1)]{Power of the test with residuals from N(0,1)}\label{fig:ex2.power2}
\end{figure}


\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}[H]

{\centering \includegraphics[width=\maxwidth]{figure/ex2_power1-1} 

}

\caption[Power of the test with residuals from random walk]{Power of the test with residuals from random walk}\label{fig:ex2.power1}
\end{figure}


\end{knitrout}

In the case of random walk even if $H_0$ is very close to the true value, the test will reject it.

\end{document}
